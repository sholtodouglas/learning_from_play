{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sholtodouglas/learning_from_play/blob/master/experimental/Generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp98H14NO9A_",
    "outputId": "7c7f5ea4-85d7-4146-84ac-c2d5282da431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 2.0MB 11.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 133kB 49.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 163kB 44.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 102kB 6.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 71kB 4.4MB/s \n",
      "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 122kB 12.6MB/s \n",
      "\u001b[?25h  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 256kB 8.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 512kB 25.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 31.8MB/s \n",
      "\u001b[?25h  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb -q\n",
    "!pip install pathy -q\n",
    "!pip install comet_ml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30D0DoKkNs06",
    "outputId": "de0dccf2-a1b6-4eec-dc1f-713e1bb06378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(actor_layer_size=2048, batch_size=512, beta=3e-05, bucket_name='iowa_bucket_lfp', colab=True, data_source='GCS', device='TPU', encoder_layer_size=512, from_tfrecords=True, gcbc=False, images=False, img_embedding_size=64, latent_dim=256, learning_rate=0.0003, num_distribs=None, planner_layer_size=512, qbits=None, resume=False, run_name='refactor_test', test_datasets=['UR5_slow_gripper_test'], tpu_name=None, train_datasets=['UR5', 'UR5_slow_gripper', 'UR5_high_transition'], train_steps=200000, window_size_max=40, window_size_min=20)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='LFP training arguments')\n",
    "parser.add_argument('run_name')\n",
    "parser.add_argument('--train_datasets', nargs='+', help='Training dataset names')\n",
    "parser.add_argument('--test_datasets', nargs='+', help='Testing dataset names')\n",
    "parser.add_argument('-c', '--colab', default=False, action='store_true', help='Enable if using colab environment')\n",
    "parser.add_argument('-s', '--data_source', default='DRIVE', help='Source of training data')\n",
    "parser.add_argument('-tfr', '--from_tfrecords', default=False, action='store_true', help='Enable if using tfrecords format')\n",
    "parser.add_argument('-d', '--device', default='TPU', help='Hardware device to train on')\n",
    "parser.add_argument('-b', '--batch_size', default=512, type=int)\n",
    "parser.add_argument('-wmax', '--window_size_max', default=40, type=int)\n",
    "parser.add_argument('-wmin', '--window_size_min', default=20, type=int)\n",
    "parser.add_argument('-la', '--actor_layer_size', default=2048, type=int, help='Layer size of actor, increases size of neural net')\n",
    "parser.add_argument('-le', '--encoder_layer_size', default=512, type=int, help='Layer size of encoder, increases size of neural net')\n",
    "parser.add_argument('-lp', '--planner_layer_size', default=512, type=int, help='Layer size of planner, increases size of neural net')\n",
    "parser.add_argument('-embd', '--img_embedding_size', default=64, type=int, help='Embedding size of features,goal space')\n",
    "parser.add_argument('-z', '--latent_dim', default=256, type=int, help='Size of the VAE latent space')\n",
    "parser.add_argument('-g', '--gcbc', default=False, action='store_true', help='Enables GCBC, a simpler model with no encoder/planner')\n",
    "parser.add_argument('-n', '--num_distribs', default=None, type=int, help='Number of distributions to use in logistic mixture model')\n",
    "parser.add_argument('-q', '--qbits', default=None, type=int, help='Number of quantisation bits to discrete distributions into. Total quantisations = 2**qbits')\n",
    "parser.add_argument('-lr', '--learning_rate', type=float, default=3e-4)\n",
    "parser.add_argument('-t', '--train_steps', type=int, default=200000)\n",
    "parser.add_argument('-r', '--resume', default=False, action='store_true')\n",
    "parser.add_argument('-B', '--beta', type=float, default=0.00003)\n",
    "parser.add_argument('-i', '--images', default=False, action='store_true')\n",
    "parser.add_argument('--bucket_name', help='GCS bucket name to stream data from')\n",
    "parser.add_argument('--tpu_name', help='GCP TPU name') # Only used in the script on GCP\n",
    "\n",
    "### Sample local config\n",
    "args = parser.parse_args('''\n",
    "dummy_run \n",
    "--train_dataset UR5\n",
    "--test_dataset UR5\n",
    "-tfr\n",
    "'''.split())\n",
    "\n",
    "## Sample colab config\n",
    "args = parser.parse_args('''\n",
    "refactor_test\n",
    "--train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
    "--test_dataset UR5_slow_gripper_test\n",
    "-c\n",
    "-tfr\n",
    "-s GCS\n",
    "-d TPU\n",
    "-b 512\n",
    "-la 2048\n",
    "-le 512\n",
    "-lp 512\n",
    "-z 256\n",
    "-lr 3e-4\n",
    "--bucket_name iowa_bucket_lfp\n",
    "'''.split())\n",
    "\n",
    "# ## Sample colab config\n",
    "# args = parser.parse_args('''\n",
    "# QuantB0_01\n",
    "# --train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
    "# --test_dataset UR5_slow_gripper_test\n",
    "# -c\n",
    "# -s DRIVE\n",
    "# -d TPU\n",
    "# -b 512\n",
    "# -la 2048\n",
    "# -le 512\n",
    "# -lp 512\n",
    "# -z 256\n",
    "# -lr 3e-4\n",
    "# -B 0.01\n",
    "# -n 5\n",
    "# -q 8\n",
    "# '''.split())\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8h1VQv0O2jv",
    "outputId": "f86a75f1-0cf5-409c-a452-374648791922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using colab setup\n",
      "Cloning into '/content/learning_from_play'...\n",
      "remote: Enumerating objects: 129, done.\u001b[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "remote: Total 2217 (delta 77), reused 82 (delta 41), pack-reused 2088\u001b[K\n",
      "Receiving objects: 100% (2217/2217), 82.48 MiB | 7.54 MiB/s, done.\n",
      "Resolving deltas: 100% (1314/1314), done.\n",
      "Mounted at /content/drive\n",
      "No pybullet installation found - which is fine if training\n",
      "Reading data from Google Drive\n",
      "Storage path: /content/drive/My Drive/Robotic Learning\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pathy import Pathy\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#@title Workpace Setup (Local vs Colab)\n",
    "\n",
    "# Set up working directory and libraries\n",
    "if args.colab:\n",
    "    from google.colab import drive, auth\n",
    "    print('Using colab setup')\n",
    "    WORKING_PATH = Path('/content/learning_from_play')\n",
    "    # Clone repo\n",
    "    try:\n",
    "        get_ipython().system(\"git clone 'https://github.com/sholtodouglas/learning_from_play' {WORKING_PATH}\")\n",
    "    except: \n",
    "        pass\n",
    "    # Mount drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print('Using local setup')\n",
    "    WORKING_PATH = Path.cwd()\n",
    "    print(f'Working path: {WORKING_PATH}')\n",
    "\n",
    "# Change working directory to learning_from_play\n",
    "os.chdir(WORKING_PATH)\n",
    "import lfp\n",
    "\n",
    "# Set up storage directory and datasets\n",
    "if args.data_source == 'DRIVE':\n",
    "    assert args.colab, \"Must be using Colab\"\n",
    "    print('Reading data from Google Drive')\n",
    "    STORAGE_PATH = Path('/content/drive/My Drive/Robotic Learning')\n",
    "elif args.data_source == 'GCS':\n",
    "    if args.colab:\n",
    "      auth.authenticate_user()\n",
    "    print('Reading data from Google Cloud Storage')\n",
    "    r = requests.get('https://ipinfo.io')\n",
    "    region = r.json()['region']\n",
    "    project_id = 'learning-from-play-303306'\n",
    "    logging.warning(f'You are accessing GCS data from {region}, make sure this is the same as your bucket {args.bucket_name}')\n",
    "    STORAGE_PATH = Pathy(f'gs://{args.bucket_name}')\n",
    "else:\n",
    "    print('Reading data from local filesystem')\n",
    "    STORAGE_PATH = WORKING_PATH\n",
    "\n",
    "print(f'Storage path: {STORAGE_PATH}')\n",
    "TRAIN_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.train_datasets]\n",
    "TEST_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.test_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpaStYUOO5El",
    "outputId": "0b42486a-c090-406a-a8b7-cf56329b579b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v7aUKhOtRdzW"
   },
   "outputs": [],
   "source": [
    "dl = lfp.data.PlayDataloader(include_imgs = args.images, batch_size=1,  window_size=args.window_size_max, min_window_size=args.window_size_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gU2nieBmU-I4",
    "outputId": "cfdd767b-ddd3-4a53-b5a5-a5a21b7cae0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'acts': TensorSpec(shape=(1, 40, 7), dtype=tf.float32, name=None),\n",
      "    'dataset_path': TensorSpec(shape=(1, None), dtype=tf.int32, name=None),\n",
      "    'goals': TensorSpec(shape=(1, 40, 11), dtype=tf.float32, name=None),\n",
      "    'masks': TensorSpec(shape=(1, 40), dtype=tf.float32, name=None),\n",
      "    'obs': TensorSpec(shape=(1, 40, 18), dtype=tf.float32, name=None),\n",
      "    'seq_lens': TensorSpec(shape=(1,), dtype=tf.float32, name=None),\n",
      "    'tstep_idxs': TensorSpec(shape=(1, None), dtype=tf.int32, name=None)}\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "train_data = dl.extract(TRAIN_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
    "train_dataset = dl.load(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewqeftlUfuNY"
   },
   "outputs": [],
   "source": [
    "def serialise(data):\n",
    "    \n",
    "    obs, acts, achieved_goals, joint_poses, target_poses, acts_quat, acts_rpy_rel, velocities, obs_quat, gripper_proprioception, sequence_index, sequence_id, img = data['obs'], data['acts'], data['achieved_goals'], data['joint_poses'], data['target_poses'], data['acts_quat'], data['acts_rpy_rel'], data['velocities'], data['obs_quat'], data['gripper_proprioception'], data['sequence_index'], data['sequence_id'], data['img']\n",
    "    \n",
    "    #image_feature = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(image).numpy(),]))\n",
    "    \n",
    "    obs = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(obs).numpy(),]))\n",
    "    acts = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(acts).numpy(),]))\n",
    "    achieved_goals = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(achieved_goals).numpy(),])) \n",
    "    joint_poses = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(joint_poses).numpy(),])) \n",
    "    target_poses = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(target_poses).numpy(),]))\n",
    "    acts_quat = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(acts_quat).numpy(),]))\n",
    "    acts_rpy_rel = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(acts_rpy_rel).numpy(),])) \n",
    "    velocities = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(velocities).numpy(),])) \n",
    "    obs_quat = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(obs_quat).numpy(),]))\n",
    "    gripper_proprioception = Feature(bytes_list=BytesList(value=[tf.io.serialize_tensor(gripper_proprioception).numpy(),])) \n",
    "    sequence_index =  Feature(int64_list=Int64List(value=[sequence_index,]))\n",
    "    sequence_id = Feature(int64_list=Int64List(value=[sequence_id,]))\n",
    "    img = Feature(bytes_list=BytesList(value=[img.numpy(),]))\n",
    "    # img is already serialised because we never decode it!\n",
    "    \n",
    "    features = Features(feature={\n",
    "                'obs': obs,\n",
    "                'acts': acts,\n",
    "                'achieved_goals': achieved_goals,\n",
    "                'joint_poses': joint_poses,\n",
    "                'target_poses': target_poses,\n",
    "                'acts_quat': acts_quat,\n",
    "                'acts_rpy_rel': acts_rpy_rel,\n",
    "                'velocities': velocities,\n",
    "                'obs_quat': obs_quat,\n",
    "                'gripper_proprioception': gripper_proprioception,\n",
    "                'sequence_index': sequence_index,\n",
    "                'sequence_id': sequence_id,\n",
    "                'img': img,\n",
    "                })\n",
    "    \n",
    "    example = Example(features=features)\n",
    "    \n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.io.TFRecordWriter(str(path/'tf_records/')+f\"/{demo}.tfrecords\") as file_writer:\n",
    "            for data in it:\n",
    "                byte_stream = serialise(data)\n",
    "                file_writer.write(byte_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZ1fcxyeiI-O"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNz/0KOI3rpk7dj0o8h79u9",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Generate_dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
