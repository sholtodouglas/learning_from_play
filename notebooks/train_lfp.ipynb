{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "LMP Test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "G84fGXVwv9Fh",
        "-yC0L1P3v9GA",
        "8SHQGBjnv9GK",
        "b7CMny5dv9Gx",
        "njbQpQAGv9G2"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sholtodouglas/learning_from_play/blob/dvae_plan/notebooks/train_lfp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jUYgZwpGfav",
        "outputId": "e24717e2-7927-4760-cd03-740165c58098"
      },
      "source": [
        "!pip install wandb -q\n",
        "!pip install pathy -q\n",
        "!pip install comet_ml -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0MB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 35.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 28.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 122kB 9.5MB/s \n",
            "\u001b[?25h  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 256kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 522kB 18.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25h  Building wheel for netifaces (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "OFJvewo0ee6r",
        "outputId": "9736a4e2-4bdd-4684-c31e-131c87d0dbb7"
      },
      "source": [
        "# import comet_ml at the top of your file\n",
        "from comet_ml import Experiment\n",
        "import wandb\n",
        "wandb.login()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z68MkXdDZaSZ",
        "outputId": "b6258e28-e8b0-451b-850d-fe07082c0c8a"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='LFP training arguments')\n",
        "parser.add_argument('run_name')\n",
        "parser.add_argument('--train_datasets', nargs='+', help='Training dataset names')\n",
        "parser.add_argument('--test_datasets', nargs='+', help='Testing dataset names')\n",
        "parser.add_argument('-c', '--colab', default=False, action='store_true', help='Enable if using colab environment')\n",
        "parser.add_argument('-s', '--data_source', default='DRIVE', help='Source of training data')\n",
        "parser.add_argument('-tfr', '--from_tfrecords', default=False, action='store_true', help='Enable if using tfrecords format')\n",
        "parser.add_argument('-d', '--device', default='TPU', help='Hardware device to train on')\n",
        "parser.add_argument('-b', '--batch_size', default=512, type=int)\n",
        "parser.add_argument('-wmax', '--window_size_max', default=50, type=int)\n",
        "parser.add_argument('-wmin', '--window_size_min', default=20, type=int)\n",
        "parser.add_argument('-la', '--actor_layer_size', default=2048, type=int, help='Layer size of actor, increases size of neural net')\n",
        "parser.add_argument('-le', '--encoder_layer_size', default=512, type=int, help='Layer size of encoder, increases size of neural net')\n",
        "parser.add_argument('-lp', '--planner_layer_size', default=512, type=int, help='Layer size of planner, increases size of neural net')\n",
        "parser.add_argument('-embd', '--img_embedding_size', default=64, type=int, help='Embedding size of features,goal space')\n",
        "parser.add_argument('-z', '--latent_dim', default=256, type=int, help='Size of the VAE latent space')\n",
        "parser.add_argument('-g', '--gcbc', default=False, action='store_true', help='Enables GCBC, a simpler model with no encoder/planner')\n",
        "parser.add_argument('-n', '--num_distribs', default=None, type=int, help='Number of distributions to use in logistic mixture model')\n",
        "parser.add_argument('-q', '--qbits', default=None, type=int, help='Number of quantisation bits to discrete distributions into. Total quantisations = 2**qbits')\n",
        "parser.add_argument('-lr', '--learning_rate', type=float, default=3e-4)\n",
        "parser.add_argument('-t', '--train_steps', type=int, default=200000)\n",
        "parser.add_argument('-r', '--resume', default=False, action='store_true')\n",
        "parser.add_argument('-B', '--beta', type=float, default=0.00003)\n",
        "parser.add_argument('-i', '--images', default=False, action='store_true')\n",
        "parser.add_argument('--fp16', default=False, action='store_true')\n",
        "parser.add_argument('--bucket_name', help='GCS bucket name to stream data from')\n",
        "parser.add_argument('--tpu_name', help='GCP TPU name') # Only used in the script on GCP\n",
        "\n",
        "\n",
        "# ## Sample colab config\n",
        "# args = parser.parse_args('''\n",
        "# refactor_test\n",
        "# --train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
        "# --test_dataset UR5_slow_gripper_test\n",
        "# -c\n",
        "# -tfr\n",
        "# -s GCS\n",
        "# -d TPU\n",
        "# -b 512\n",
        "# -la 2048\n",
        "# -le 512\n",
        "# -lp 512\n",
        "# -z 256\n",
        "# -lr 3e-4\n",
        "# --bucket_name iowa_bucket_lfp\n",
        "# -i\n",
        "# '''.split())\n",
        "\n",
        "## Sample colab config\n",
        "args = parser.parse_args('''\n",
        "QuantB0_01\n",
        "--train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
        "--test_dataset UR5_slow_gripper_test\n",
        "-c\n",
        "-s DRIVE\n",
        "-d TPU\n",
        "-b 512\n",
        "-la 2048\n",
        "-le 512\n",
        "-lp 512\n",
        "-z 256\n",
        "-lr 3e-4\n",
        "-B 0.0\n",
        "'''.split())\n",
        "\n",
        "# -n 5\n",
        "# -q 8\n",
        "\n",
        "print(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(actor_layer_size=2048, batch_size=512, beta=0.01, bucket_name=None, colab=True, data_source='DRIVE', device='TPU', encoder_layer_size=512, fp16=False, from_tfrecords=False, gcbc=False, images=False, img_embedding_size=64, latent_dim=256, learning_rate=0.0003, num_distribs=None, planner_layer_size=512, qbits=None, resume=False, run_name='QuantB0_01', test_datasets=['UR5_slow_gripper_test'], tpu_name=None, train_datasets=['UR5', 'UR5_slow_gripper', 'UR5_high_transition'], train_steps=200000, window_size_max=50, window_size_min=20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EliqxOpPv9Dy"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMJ___6ARZ4u",
        "outputId": "01142b41-3245-4213-d9c6-199f728d09f0"
      },
      "source": [
        "from pathlib import Path\n",
        "from pathy import Pathy\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import pprint\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "#@title Workpace Setup (Local vs Colab)\n",
        "\n",
        "# Set up working directory and libraries\n",
        "if args.colab:\n",
        "    from google.colab import drive, auth\n",
        "    print('Using colab setup')\n",
        "    WORKING_PATH = Path('/content/learning_from_play')\n",
        "    # Clone repo\n",
        "    try:\n",
        "        get_ipython().system(\"git clone 'https://github.com/sholtodouglas/learning_from_play' {WORKING_PATH}\")\n",
        "    except: \n",
        "        pass\n",
        "    # Mount drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('Using local setup')\n",
        "    WORKING_PATH = Path.cwd()\n",
        "    print(f'Working path: {WORKING_PATH}')\n",
        "\n",
        "# Change working directory to learning_from_play\n",
        "os.chdir(WORKING_PATH)\n",
        "import lfp\n",
        "\n",
        "# Set up storage directory and datasets\n",
        "if args.data_source == 'DRIVE':\n",
        "    assert args.colab, \"Must be using Colab\"\n",
        "    print('Reading data from Google Drive')\n",
        "    STORAGE_PATH = Path('/content/drive/My Drive/Robotic Learning')\n",
        "elif args.data_source == 'GCS':\n",
        "    if args.colab:\n",
        "      auth.authenticate_user()\n",
        "    print('Reading data from Google Cloud Storage')\n",
        "    r = requests.get('https://ipinfo.io')\n",
        "    region = r.json()['region']\n",
        "    project_id = 'learning-from-play-303306'\n",
        "    logging.warning(f'You are accessing GCS data from {region}, make sure this is the same as your bucket {args.bucket_name}')\n",
        "    STORAGE_PATH = Pathy(f'gs://{args.bucket_name}')\n",
        "else:\n",
        "    print('Reading data from local filesystem')\n",
        "    STORAGE_PATH = WORKING_PATH\n",
        "\n",
        "print(f'Storage path: {STORAGE_PATH}')\n",
        "TRAIN_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.train_datasets]\n",
        "TEST_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.test_datasets]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using colab setup\n",
            "Cloning into '/content/learning_from_play'...\n",
            "remote: Enumerating objects: 226, done.\u001b[K\n",
            "remote: Counting objects: 100% (226/226), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 2314 (delta 122), reused 153 (delta 66), pack-reused 2088\u001b[K\n",
            "Receiving objects: 100% (2314/2314), 95.37 MiB | 27.28 MiB/s, done.\n",
            "Resolving deltas: 100% (1359/1359), done.\n",
            "Mounted at /content/drive\n",
            "No pybullet installation found - which is fine if training\n",
            "Reading data from Google Drive\n",
            "Storage path: /content/drive/My Drive/Robotic Learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zbmq324x2yv",
        "outputId": "361feab5-747e-42c9-ee10-1e681d98cd4d"
      },
      "source": [
        "\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "if args.device == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu_name)  # TPU detection\n",
        "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    except ValueError:\n",
        "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    NUM_DEVICES = strategy.num_replicas_in_sync\n",
        "    print(\"REPLICAS: \", NUM_DEVICES)\n",
        "    if args.fp16:\n",
        "        tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
        "else:\n",
        "    physical_devices = tf.config.list_physical_devices()\n",
        "    if args.device == 'GPU':\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[3], enable=True)\n",
        "        if args.fp16:\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    NUM_DEVICES = 1\n",
        "    print(physical_devices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n",
            "Running on TPU  ['10.95.255.170:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.95.255.170:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.95.255.170:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HejtDH_Yx8h",
        "outputId": "9a6b5656-0211-4fa5-f85a-9157e2610330"
      },
      "source": [
        "# Use this to edit modules without needing to restart the kernel (can also edit local, push/pull)\n",
        "!git pull\n",
        "import importlib\n",
        "importlib.reload(lfp.data)\n",
        "importlib.reload(lfp.model)\n",
        "importlib.reload(lfp.train)\n",
        "importlib.reload(lfp.metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 6), reused 8 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "From https://github.com/sholtodouglas/learning_from_play\n",
            "   b4427ea..f7beb96  master     -> origin/master\n",
            "Updating b4427ea..f7beb96\n",
            "Fast-forward\n",
            " lfp/plotting.py        |  36 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " lfp/train.py           |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " notebooks/Deploy.ipynb | 712 \u001b[32m++++++++++++++++\u001b[m\u001b[31m---------------------------------\u001b[m\n",
            " train_lfp.py           |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 4 files changed, 261 insertions(+), 495 deletions(-)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'lfp.metric' from '/content/learning_from_play/lfp/metric.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUI6JP06FdTv"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2lBWIyOFdTv"
      },
      "source": [
        "GLOBAL_BATCH_SIZE = args.batch_size * NUM_DEVICES\n",
        "dl = lfp.data.PlayDataloader(include_imgs = args.images, batch_size=GLOBAL_BATCH_SIZE,  window_size=args.window_size_max, min_window_size=args.window_size_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiz4AXCnFdTv",
        "outputId": "e7a7d598-4065-4f75-b6ab-47790608b030"
      },
      "source": [
        "# Train data\n",
        "train_data = dl.extract(TRAIN_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
        "train_dataset = dl.load(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UR5: 100%|██████████| 52/52 [00:21<00:00,  2.45it/s]\n",
            "UR5_slow_gripper: 100%|██████████| 23/23 [00:09<00:00,  2.46it/s]\n",
            "UR5_high_transition: 100%|██████████| 32/32 [00:11<00:00,  2.74it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{   'acts': TensorSpec(shape=(4096, 50, 7), dtype=tf.float32, name=None),\n",
            "    'dataset_path': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None),\n",
            "    'goals': TensorSpec(shape=(4096, 50, 11), dtype=tf.float32, name=None),\n",
            "    'masks': TensorSpec(shape=(4096, 50), dtype=tf.float32, name=None),\n",
            "    'obs': TensorSpec(shape=(4096, 50, 18), dtype=tf.float32, name=None),\n",
            "    'seq_lens': TensorSpec(shape=(4096,), dtype=tf.float32, name=None),\n",
            "    'tstep_idxs': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ZDAVpLFdTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa2ba76-e05e-41c5-b5ca-0ae36517f0ab"
      },
      "source": [
        "# Validation data\n",
        "valid_data = dl.extract(TEST_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
        "valid_dataset = dl.load(valid_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UR5_slow_gripper_test: 100%|██████████| 2/2 [00:01<00:00,  1.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{   'acts': TensorSpec(shape=(4096, 50, 7), dtype=tf.float32, name=None),\n",
            "    'dataset_path': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None),\n",
            "    'goals': TensorSpec(shape=(4096, 50, 11), dtype=tf.float32, name=None),\n",
            "    'masks': TensorSpec(shape=(4096, 50), dtype=tf.float32, name=None),\n",
            "    'obs': TensorSpec(shape=(4096, 50, 18), dtype=tf.float32, name=None),\n",
            "    'seq_lens': TensorSpec(shape=(4096,), dtype=tf.float32, name=None),\n",
            "    'tstep_idxs': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWIpRPRuv9E6",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uMXBjGzgZCA"
      },
      "source": [
        "from lfp.train import LFPTrainer\n",
        "\n",
        "def train_setup():\n",
        "    model_params = {'obs_dim':args.img_embedding_size + dl.proprioceptive_features_dim if args.images else dl.obs_dim,\n",
        "                'goal_dim':args.img_embedding_size if args.images else dl.goal_dim,\n",
        "                'act_dim':dl.act_dim,\n",
        "                'layer_size':args.actor_layer_size, \n",
        "                'latent_dim':args.latent_dim}\n",
        "\n",
        "    actor = lfp.model.create_actor(**model_params, gcbc=args.gcbc, num_distribs=args.num_distribs, qbits=args.qbits)\n",
        "\n",
        "    if args.gcbc:\n",
        "        encoder = None\n",
        "        planner = None\n",
        "    else:\n",
        "        model_params['layer_size'] = args.encoder_layer_size\n",
        "        encoder = lfp.model.create_encoder(**model_params)\n",
        "        model_params['layer_size'] = args.planner_layer_size\n",
        "        planner = lfp.model.create_planner(**model_params)\n",
        "\n",
        "    if args.images:\n",
        "      cnn = lfp.model.create_vision_network(dl.img_size, dl.img_size, embedding_size=args.img_embedding_size)\n",
        "    else:\n",
        "      cnn = None\n",
        "\n",
        "    #optimizer = tfa.optimizers.LAMB(learning_rate=args.learning_rate)\n",
        "    optimizer = optimizer = tf.optimizers.Adam\n",
        "    trainer = LFPTrainer(args, actor, dl, encoder, planner, cnn, optimizer, strategy, GLOBAL_BATCH_SIZE)\n",
        "    return actor, encoder, planner, trainer\n",
        "\n",
        "\n",
        "if args.device=='CPU' or args.device=='GPU':\n",
        "     actor, encoder, planner, trainer = train_setup()\n",
        "else:\n",
        "    with strategy.scope():\n",
        "         actor, encoder, planner, trainer = train_setup()\n",
        "        \n",
        "        \n",
        "train_dist_dataset = iter(strategy.experimental_distribute_dataset(train_dataset))\n",
        "valid_dist_dataset = iter(strategy.experimental_distribute_dataset(valid_dataset))\n",
        "plotting_dataset = iter(valid_dataset) #for the cluster fig, easier with a non distributed dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtSlIEoBlDyp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "bb11188c-9d58-481c-abd0-6f502ff783a5"
      },
      "source": [
        "from lfp.train import BetaScheduler\n",
        "\n",
        "\n",
        "# 0.00001 so far works best for MAE - try lower - 0.00003 with a BETA accel of 10 so far works best, perfect encoder, nicely mapped planner\n",
        "# recall 0.01 worked okay for probabilistic - proba still too unstable!\n",
        "beta_sched = BetaScheduler('linear', \n",
        "                           beta=args.beta, \n",
        "                           beta_max=args.beta, \n",
        "                           max_steps=args.train_steps, \n",
        "                           cycles=90, \n",
        "                           duty_cycle=0.5\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhSGD_2imWDi"
      },
      "source": [
        "\n",
        "from tensorflow.keras.utils import Progbar\n",
        "progbar = Progbar(args.train_steps, verbose=1, interval=0.5)\n",
        "valid_inc = 20\n",
        "save_inc = 2000\n",
        "prev_grad_norm = np.float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTyriZrYowMJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "7b6e8315-86c7-41c7-d1ee-41a8685c0fad"
      },
      "source": [
        "run_name = args.run_name\n",
        "model_path = str(STORAGE_PATH/'saved_models'/args.run_name)\n",
        "\n",
        "if args.resume:\n",
        "  # WandB reinit\n",
        "  with open(f'{model_path}/config.json', 'r') as f:\n",
        "      data = json.load(f)\n",
        "  # Comet ML reinit\n",
        "  exp = ExistingExperiment(api_key=\"C4vcCM57bnSYEsdncguxDW8pO\",  previous_experiment=data['experiment_key'])\n",
        "\n",
        "  wandb.init(project=\"learning-from-play_v2\", id=data['run_id'],  resume=\"must\")\n",
        "  t = wandb.run.step + valid_inc # Todo get this from comet to complete the transition\n",
        "\n",
        "  load_weights(model_path, actor, encoder, planner, with_optimizer=True)\n",
        "  print('Loaded model weights and optimiser state')\n",
        "  \n",
        "  prognar.add(t, []) # update the progbar to the most recent point\n",
        "else:\n",
        "  #Comet\n",
        "  experiment = Experiment(api_key=\"C4vcCM57bnSYEsdncguxDW8pO\",project_name=\"learning-from-play\",workspace=\"sholtodouglas\")\n",
        "  experiment.set_name(run_name)\n",
        "  # WandB\n",
        "  wandb.init(project=\"learning-from-play_v2\")\n",
        "  wandb.run.name = run_name\n",
        "  t = 0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/sholtodouglas/learning-from-play/805392ffef944410babfd717bacc3b2e\n",
            "\n",
            "wandb: Currently logged in as: sholto (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">azure-durian-201</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sholto/learning-from-play_v2\" target=\"_blank\">https://wandb.ai/sholto/learning-from-play_v2</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/sholto/learning-from-play_v2/runs/v2p8rhfo\" target=\"_blank\">https://wandb.ai/sholto/learning-from-play_v2/runs/v2p8rhfo</a><br/>\n",
              "                Run data is saved locally in <code>/content/learning_from_play/wandb/run-20210306_133913-v2p8rhfo</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjnc22FOGae1"
      },
      "source": [
        "wfrom lfp.plotting import produce_cluster_fig, project_enc_and_plan, plot_to_image\n",
        "from lfp.metric import log # gets state and clears simultaneously"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXF0Y8aKrsng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "546cd282-5919-4e81-dd4c-e6ac36cbd4ff"
      },
      "source": [
        "\n",
        "\n",
        "# Creating these autograph wrappers so that tf.data operations are executed in graph mode\n",
        "@tf.function\n",
        "def train(train_dataset, beta):\n",
        "    train_batch = next(train_dataset)\n",
        "    trainer.distributed_train_step(train_batch, beta)\n",
        "\n",
        "@tf.function\n",
        "def test(valid_dataset, beta):\n",
        "    valid_batch = next(valid_dataset)\n",
        "    trainer.distributed_test_step(valid_batch, beta)\n",
        "\n",
        "while t < args.train_steps:\n",
        "    start_time = time.time()\n",
        "    beta = beta_sched.scheduler(t)\n",
        "    train(train_dist_dataset, beta)\n",
        "\n",
        "    if t % valid_inc == 0:\n",
        "        test(valid_dist_dataset, beta)\n",
        "        step_time = round(time.time() - start_time, 1)\n",
        "\n",
        "        metrics = {metric_name: log(metric) for metric_name, metric in trainer.metrics.items()}\n",
        "        metrics['step_time'] = step_time\n",
        "\n",
        "        # validation plotting\n",
        "        progbar.add(valid_inc, [('Train Loss', metrics['train_loss']),\n",
        "                                ('Validation Loss', metrics['valid_loss']),\n",
        "                                ('Time (s)', step_time)])\n",
        "        #Plot on Comet\n",
        "        experiment.log_metrics(metrics,step=t)\n",
        "        # Plot on WandB\n",
        "        wandb.log(metrics, step=t)\n",
        "\n",
        "    if (t+1) % save_inc == 0:\n",
        "        trainer.save_weights(model_path, run_id=wandb.run.id, experiment_key=experiment.get_key())\n",
        "        if not args.gcbc and not args.images:\n",
        "          z_enc, z_plan = produce_cluster_fig(next(plotting_dataset), encoder, planner, TEST_DATA_PATHS[0], num_take=dl.batch_size//4)\n",
        "\n",
        "          #Comet\n",
        "          experiment.log_figure('z_enc', z_enc, step=t)\n",
        "          experiment.log_figure('z_plan', z_plan,step=t)\n",
        "\n",
        "          # WandB\n",
        "          wandb.log({'z_enc':z_enc, 'z_plan':z_plan}, step=t)\n",
        "\n",
        "          #latent_fig = project_enc_and_plan(ze, zp)\n",
        "          #latent_img = plot_to_image(latent_fig)\n",
        "\n",
        "    t += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    60/200000 [..............................] - ETA: 409:30:55 - Train Loss: 0.0339 - Validation Loss: 0.0255 - Time (s): 75.9667 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-86281f5d3dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta_sched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalid_inc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw21h14BnX3Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}