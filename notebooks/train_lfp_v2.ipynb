{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "LMP Test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "G84fGXVwv9Fh",
        "-yC0L1P3v9GA",
        "8SHQGBjnv9GK",
        "b7CMny5dv9Gx",
        "njbQpQAGv9G2"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sholtodouglas/learning_from_play/blob/train_loop_refactor/notebooks/train_lfp_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jUYgZwpGfav",
        "outputId": "f8a2257a-48a4-49e0-bee9-aacf1f600114"
      },
      "source": [
        "!pip install -q wandb pathy comet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.1MB 7.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 45.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.7MB/s \n",
            "\u001b[?25h  Building wheel for comet (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFJvewo0ee6r",
        "outputId": "879b6022-594d-4657-fa59-9f6c6cdd5110"
      },
      "source": [
        "# import comet_ml at the top of your file\n",
        "from comet_ml import Experiment\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtfrizza\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z68MkXdDZaSZ",
        "outputId": "e2e8d325-bee3-402b-f685-df76df676864"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='LFP training arguments')\n",
        "parser.add_argument('run_name')\n",
        "parser.add_argument('--train_datasets', nargs='+', help='Training dataset names')\n",
        "parser.add_argument('--test_datasets', nargs='+', help='Testing dataset names')\n",
        "parser.add_argument('-c', '--colab', default=False, action='store_true', help='Enable if using colab environment')\n",
        "parser.add_argument('-s', '--data_source', default='DRIVE', help='Source of training data')\n",
        "parser.add_argument('-tfr', '--from_tfrecords', default=False, action='store_true', help='Enable if using tfrecords format')\n",
        "parser.add_argument('-d', '--device', default='TPU', help='Hardware device to train on')\n",
        "parser.add_argument('-b', '--batch_size', default=512, type=int)\n",
        "parser.add_argument('-wmax', '--window_size_max', default=50, type=int)\n",
        "parser.add_argument('-wmin', '--window_size_min', default=20, type=int)\n",
        "parser.add_argument('-la', '--actor_layer_size', default=2048, type=int, help='Layer size of actor, increases size of neural net')\n",
        "parser.add_argument('-le', '--encoder_layer_size', default=512, type=int, help='Layer size of encoder, increases size of neural net')\n",
        "parser.add_argument('-lp', '--planner_layer_size', default=512, type=int, help='Layer size of planner, increases size of neural net')\n",
        "parser.add_argument('-embd', '--img_embedding_size', default=64, type=int, help='Embedding size of features,goal space')\n",
        "parser.add_argument('-z', '--latent_dim', default=256, type=int, help='Size of the VAE latent space')\n",
        "parser.add_argument('-g', '--gcbc', default=False, action='store_true', help='Enables GCBC, a simpler model with no encoder/planner')\n",
        "parser.add_argument('-n', '--num_distribs', default=None, type=int, help='Number of distributions to use in logistic mixture model')\n",
        "parser.add_argument('-q', '--qbits', default=None, type=int, help='Number of quantisation bits to discrete distributions into. Total quantisations = 2**qbits')\n",
        "parser.add_argument('-lr', '--learning_rate', type=float, default=3e-4)\n",
        "parser.add_argument('-t', '--train_steps', type=int, default=200000)\n",
        "parser.add_argument('-r', '--resume', default=False, action='store_true')\n",
        "parser.add_argument('-B', '--beta', type=float, default=0.00003)\n",
        "parser.add_argument('-i', '--images', default=False, action='store_true')\n",
        "parser.add_argument('--fp16', default=False, action='store_true')\n",
        "parser.add_argument('--bucket_name', help='GCS bucket name to stream data from')\n",
        "parser.add_argument('--tpu_name', help='GCP TPU name') # Only used in the script on GCP\n",
        "\n",
        "\n",
        "# ## Sample colab config\n",
        "# args = parser.parse_args('''\n",
        "# refactor_test\n",
        "# --train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
        "# --test_dataset UR5_slow_gripper_test\n",
        "# -c\n",
        "# -tfr\n",
        "# -s GCS\n",
        "# -d TPU\n",
        "# -b 512\n",
        "# -la 2048\n",
        "# -le 512\n",
        "# -lp 512\n",
        "# -z 256\n",
        "# -lr 3e-4\n",
        "# --bucket_name iowa_bucket_lfp\n",
        "# -i\n",
        "# '''.split())\n",
        "\n",
        "## Sample colab config\n",
        "args = parser.parse_args('''\n",
        "fit_test\n",
        "--train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
        "--test_dataset UR5_slow_gripper_test\n",
        "-c\n",
        "-s DRIVE\n",
        "-d TPU\n",
        "-b 512\n",
        "-la 2048\n",
        "-le 512\n",
        "-lp 512\n",
        "-z 256\n",
        "-lr 3e-4\n",
        "-B 0.01\n",
        "'''.split())\n",
        "\n",
        "# -n 5\n",
        "# -q 8\n",
        "\n",
        "print(args)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(actor_layer_size=2048, batch_size=512, beta=0.01, bucket_name=None, colab=True, data_source='DRIVE', device='TPU', encoder_layer_size=512, fp16=False, from_tfrecords=False, gcbc=False, images=False, img_embedding_size=64, latent_dim=256, learning_rate=0.0003, num_distribs=None, planner_layer_size=512, qbits=None, resume=False, run_name='fit_test', test_datasets=['UR5_slow_gripper_test'], tpu_name=None, train_datasets=['UR5', 'UR5_slow_gripper', 'UR5_high_transition'], train_steps=200000, window_size_max=50, window_size_min=20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EliqxOpPv9Dy"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMJ___6ARZ4u",
        "outputId": "3ee7f98e-5046-4075-a1a8-7ccdd9dba759"
      },
      "source": [
        "from pathlib import Path\n",
        "from pathy import Pathy\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import pprint\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "#@title Workpace Setup (Local vs Colab)\n",
        "\n",
        "# Set up working directory and libraries\n",
        "if args.colab:\n",
        "    from google.colab import drive, auth\n",
        "    print('Using colab setup')\n",
        "    WORKING_PATH = Path('/content/learning_from_play')\n",
        "    # Clone repo\n",
        "    try:\n",
        "        get_ipython().system(\"git clone 'https://github.com/sholtodouglas/learning_from_play' {WORKING_PATH}\")\n",
        "    except: \n",
        "        pass\n",
        "    # Mount drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('Using local setup')\n",
        "    WORKING_PATH = Path.cwd()\n",
        "    print(f'Working path: {WORKING_PATH}')\n",
        "\n",
        "# Change working directory to learning_from_play\n",
        "os.chdir(WORKING_PATH)\n",
        "import lfp\n",
        "\n",
        "# Set up storage directory and datasets\n",
        "if args.data_source == 'DRIVE':\n",
        "    assert args.colab, \"Must be using Colab\"\n",
        "    print('Reading data from Google Drive')\n",
        "    STORAGE_PATH = Path('/content/drive/My Drive/Robotic Learning')\n",
        "elif args.data_source == 'GCS':\n",
        "    if args.colab:\n",
        "      auth.authenticate_user()\n",
        "    print('Reading data from Google Cloud Storage')\n",
        "    r = requests.get('https://ipinfo.io')\n",
        "    region = r.json()['region']\n",
        "    project_id = 'learning-from-play-303306'\n",
        "    logging.warning(f'You are accessing GCS data from {region}, make sure this is the same as your bucket {args.bucket_name}')\n",
        "    STORAGE_PATH = Pathy(f'gs://{args.bucket_name}')\n",
        "else:\n",
        "    print('Reading data from local filesystem')\n",
        "    STORAGE_PATH = WORKING_PATH\n",
        "\n",
        "print(f'Storage path: {STORAGE_PATH}')\n",
        "TRAIN_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.train_datasets]\n",
        "TEST_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.test_datasets]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using colab setup\n",
            "fatal: destination path '/content/learning_from_play' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "No pybullet installation found - which is fine if training\n",
            "Reading data from Google Drive\n",
            "Storage path: /content/drive/My Drive/Robotic Learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zbmq324x2yv",
        "outputId": "5b917763-7d40-47d7-e3de-efd7cf6b4d5c"
      },
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "if args.device == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu_name)  # TPU detection\n",
        "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    except ValueError:\n",
        "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    NUM_DEVICES = strategy.num_replicas_in_sync\n",
        "    print(\"REPLICAS: \", NUM_DEVICES)\n",
        "    if args.fp16:\n",
        "        tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
        "else:\n",
        "    physical_devices = tf.config.list_physical_devices()\n",
        "    if args.device == 'GPU':\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[3], enable=True)\n",
        "        if args.fp16:\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    NUM_DEVICES = 1\n",
        "    print(physical_devices)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n",
            "Running on TPU  ['10.109.249.170:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.109.249.170:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.109.249.170:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HejtDH_Yx8h",
        "outputId": "9a6b5656-0211-4fa5-f85a-9157e2610330"
      },
      "source": [
        "# Use this to edit modules without needing to restart the kernel (can also edit local, push/pull)\n",
        "!git pull\n",
        "import importlib\n",
        "importlib.reload(lfp.data)\n",
        "importlib.reload(lfp.model)\n",
        "importlib.reload(lfp.train)\n",
        "importlib.reload(lfp.metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 6), reused 8 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "From https://github.com/sholtodouglas/learning_from_play\n",
            "   b4427ea..f7beb96  master     -> origin/master\n",
            "Updating b4427ea..f7beb96\n",
            "Fast-forward\n",
            " lfp/plotting.py        |  36 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " lfp/train.py           |   6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " notebooks/Deploy.ipynb | 712 \u001b[32m++++++++++++++++\u001b[m\u001b[31m---------------------------------\u001b[m\n",
            " train_lfp.py           |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 4 files changed, 261 insertions(+), 495 deletions(-)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'lfp.metric' from '/content/learning_from_play/lfp/metric.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUI6JP06FdTv"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2lBWIyOFdTv"
      },
      "source": [
        "GLOBAL_BATCH_SIZE = args.batch_size * NUM_DEVICES\n",
        "dl = lfp.data.PlayDataloader(include_imgs = args.images, batch_size=GLOBAL_BATCH_SIZE,  window_size=args.window_size_max, min_window_size=args.window_size_min)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiz4AXCnFdTv",
        "outputId": "e014275f-31e1-463b-e08d-8ae9c46ce272"
      },
      "source": [
        "# Train data\n",
        "train_data = dl.extract(TRAIN_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
        "train_dataset = dl.load(train_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UR5: 100%|██████████| 52/52 [00:01<00:00, 45.20it/s]\n",
            "UR5_slow_gripper: 100%|██████████| 23/23 [00:00<00:00, 42.45it/s]\n",
            "UR5_high_transition: 100%|██████████| 32/32 [00:00<00:00, 48.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{   'acts': TensorSpec(shape=(4096, 50, 7), dtype=tf.float32, name=None),\n",
            "    'dataset_path': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None),\n",
            "    'goals': TensorSpec(shape=(4096, 50, 11), dtype=tf.float32, name=None),\n",
            "    'masks': TensorSpec(shape=(4096, 50), dtype=tf.float32, name=None),\n",
            "    'obs': TensorSpec(shape=(4096, 50, 18), dtype=tf.float32, name=None),\n",
            "    'seq_lens': TensorSpec(shape=(4096,), dtype=tf.float32, name=None),\n",
            "    'tstep_idxs': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ZDAVpLFdTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848dde73-6240-4440-f084-d55ba731745e"
      },
      "source": [
        "# Validation data\n",
        "valid_data = dl.extract(TEST_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
        "valid_dataset = dl.load(valid_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UR5_slow_gripper_test: 100%|██████████| 2/2 [00:00<00:00, 38.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{   'acts': TensorSpec(shape=(4096, 50, 7), dtype=tf.float32, name=None),\n",
            "    'dataset_path': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None),\n",
            "    'goals': TensorSpec(shape=(4096, 50, 11), dtype=tf.float32, name=None),\n",
            "    'masks': TensorSpec(shape=(4096, 50), dtype=tf.float32, name=None),\n",
            "    'obs': TensorSpec(shape=(4096, 50, 18), dtype=tf.float32, name=None),\n",
            "    'seq_lens': TensorSpec(shape=(4096,), dtype=tf.float32, name=None),\n",
            "    'tstep_idxs': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWIpRPRuv9E6",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmVwdAknhGrS"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Input, LSTM, Concatenate, Masking, Reshape, Lambda, \\\n",
        "    Bidirectional, GRU, LayerNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.metrics import Mean\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tfpl = tfp.layers\n",
        "\n",
        "ACT_LIMITS = tf.constant([1.5, 1.5, 2.2, 3.2, 3.2, 3.2, 1.1])\n",
        "\n",
        "def create_actor(obs_dim, act_dim, goal_dim, layer_size=1024, latent_dim=256, training=True):\n",
        "    # params #\n",
        "    batch_size = None if training else 1\n",
        "    stateful = not training\n",
        "\n",
        "    # Input #\n",
        "    o = Input(shape=(None, obs_dim), batch_size=batch_size, dtype=tf.float32, name='input_obs')\n",
        "    z = Input(shape=(None, latent_dim), batch_size=batch_size, dtype=tf.float32, name='input_latent')\n",
        "    g = Input(shape=(None, goal_dim), batch_size=batch_size, dtype=tf.float32, name='input_goals')\n",
        "\n",
        "    # RNN #\n",
        "    x = Concatenate(axis=-1)([o, z, g])\n",
        "    x = Masking(mask_value=0.)(x)\n",
        "    x = LSTM(layer_size, return_sequences=True, stateful=stateful, name='LSTM_in_1')(x)\n",
        "    x = LSTM(layer_size, return_sequences=True, stateful=stateful, name='LSTM_in_2')(x)\n",
        "\n",
        "    # Deterministic output #\n",
        "    actions = Dense(act_dim, activation='tanh', name='acts')(x)\n",
        "    actions = Lambda(lambda a: a * ACT_LIMITS)(actions) # scale to action limits\n",
        "    return Model([o, z, g], actions)\n",
        "\n",
        "\n",
        "def create_encoder(obs_dim, act_dim, layer_size=2048, latent_dim=256):\n",
        "    # Input #\n",
        "    obs = Input(shape=(None, obs_dim), dtype=tf.float32, name='obs')\n",
        "    acts = Input(shape=(None, act_dim), dtype=tf.float32, name='acts')\n",
        "\n",
        "    # Layers #\n",
        "    x = Concatenate(axis=-1)([obs, acts])\n",
        "    x = Masking(mask_value=0.)(x)\n",
        "    x = Bidirectional(LSTM(layer_size, return_sequences=True), merge_mode='concat')(x)\n",
        "    x = Bidirectional(LSTM(layer_size, return_sequences=False), merge_mode='concat')(x)\n",
        "\n",
        "    # Latent Variable #\n",
        "    x = Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None)(x),\n",
        "    z = tfpl.MultivariateNormalTriL(latent_dim, name='latent')(x)\n",
        "    return Model([obs, acts], z)\n",
        "\n",
        "\n",
        "def create_planner(obs_dim, goal_dim, layer_size=2048, latent_dim=256):\n",
        "    # params #\n",
        "    batch_size = None\n",
        "\n",
        "    # Input #\n",
        "    o_i = Input(shape=(obs_dim,), batch_size=batch_size, dtype=tf.float32,\n",
        "                name='initial_obs')  # has arm state\n",
        "    o_g = Input(shape=(goal_dim,), batch_size=batch_size, dtype=tf.float32,\n",
        "                name='goal_obs')  # does not have arm state\n",
        "\n",
        "    # Layers #\n",
        "    x = Concatenate(axis=-1)([o_i, o_g])\n",
        "    x = Masking(mask_value=0.)(x)\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_1')(x) # maybe change to selu/gelu/swish?\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_2')(x)\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_3')(x)\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_4')(x)\n",
        "\n",
        "    # Latent Variable #\n",
        "    x = Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None)(x),\n",
        "    z = tfpl.MultivariateNormalTriL(latent_dim, name='latent')(x)\n",
        "    return Model([o_i, o_g], z)\n",
        "\n",
        "# Todo: add beta callback, add checkpointing callback, think about train=False autoregressive, what to do about masking?\n",
        "class LFPNet(Model):\n",
        "    def __init__(self, encoder, planner, actor, beta) -> None:\n",
        "        super(LFPNet, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.planner = planner\n",
        "        self.actor = actor\n",
        "        self.beta = beta\n",
        "        self.total_loss_tracker = Mean(name=\"total_loss\")\n",
        "        self.action_loss_tracker = Mean(name=\"action_loss\")\n",
        "        self.reg_loss_tracker = Mean(name=\"reg_loss\")\n",
        "\n",
        "    def call(self, inputs, planner=True, training=False):\n",
        "        if planner:\n",
        "            z = self.planner([inputs['obs'][:,0,:], inputs['goals'][:,0,:]])\n",
        "        else:\n",
        "            z = self.encoder([inputs['obs'], inputs['acts']])\n",
        "        z_tiled = tf.tile(tf.expand_dims(z[0], 1), (1, inputs['obs'].shape[1], 1))\n",
        "        acts = self.actor([inputs['obs'], z_tiled, inputs['goals']])\n",
        "        return acts, z\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts_enc, z_enc = self(inputs, planner=False, training=True)\n",
        "            acts_plan, z_plan = self(inputs, planner=True, training=True)\n",
        "            act_loss = self.compiled_loss(inputs['acts'], acts_enc, regularization_losses=self.losses)\n",
        "            reg_loss = tfd.kl_divergence(z_enc, z_plan)\n",
        "            loss = act_loss + self.beta * reg_loss\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.total_loss_tracker.update_state(loss)\n",
        "        self.action_loss_tracker.update_state(act_loss)\n",
        "        self.reg_loss_tracker.update_state(reg_loss)\n",
        "        result = {m.name: m.result() for m in self.metrics}\n",
        "        result['beta'] = self.beta\n",
        "        return result\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        acts_enc, z_enc = self(inputs, planner=False, training=False)\n",
        "        acts_plan, z_plan = self(inputs, planner=True, training=False)\n",
        "        act_loss = self.compiled_loss(inputs['acts'], acts_plan, regularization_losses=self.losses)\n",
        "        reg_loss = tfd.kl_divergence(z_enc, z_plan)\n",
        "        loss = act_loss + self.beta * reg_loss\n",
        "\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.total_loss_tracker.update_state(loss)\n",
        "        self.action_loss_tracker.update_state(act_loss)\n",
        "        self.reg_loss_tracker.update_state(reg_loss)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.action_loss_tracker,\n",
        "            self.reg_loss_tracker\n",
        "        ]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWFLZsyahm1X",
        "outputId": "a6fa84b3-e541-4e63-9b91-7395bd73caf8"
      },
      "source": [
        "with strategy.scope():\n",
        "    actor = create_actor(obs_dim=dl.obs_dim, act_dim=dl.act_dim, goal_dim=dl.goal_dim, layer_size=args.actor_layer_size, latent_dim=args.latent_dim)\n",
        "    encoder = create_encoder(obs_dim=dl.obs_dim, act_dim=dl.act_dim, layer_size=args.encoder_layer_size, latent_dim=args.latent_dim)\n",
        "    planner = create_planner(obs_dim=dl.obs_dim, goal_dim=dl.goal_dim, layer_size=args.encoder_layer_size, latent_dim=args.latent_dim)\n",
        "\n",
        "    model = LFPNet(encoder, planner, actor, beta=args.beta)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(args.learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse', steps_per_execution=1, run_eagerly=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not pass `graph_parents`.  They will  no longer be used.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not pass `graph_parents`.  They will  no longer be used.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2utKU_E_ws2T"
      },
      "source": [
        "class BetaSchedulerCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\" For some reason this doesn't seem to work :/ \"\"\"\n",
        "    def __init__(self, schedule):\n",
        "        super(BetaSchedulerCallback, self).__init__()\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def on_train_batch_begin(self, step, logs=None):\n",
        "        if not hasattr(self.model, \"beta\"):\n",
        "            raise ValueError('Optimizer must have a \"beta\" attribute.')\n",
        "        # Get the current learning rate from model's optimizer.\n",
        "        # beta = float(tf.keras.backend.get_value(self.model.beta))\n",
        "        # Call schedule function to get the scheduled learning rate.\n",
        "        scheduled_beta = float(self.schedule(step))\n",
        "        # Set the value back to the optimizer before this epoch starts\n",
        "        # tf.keras.backend.set_value(self.model.beta, scheduled_beta)\n",
        "        self.model.beta = scheduled_beta\n",
        "        print(f\"\\nStep {step:05}: Beta is {self.model.beta:.1e}.\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "mRiAk4pzxKSF",
        "outputId": "d501cf21-3938-4764-b463-0fb66060968f"
      },
      "source": [
        "beta_schedule = lfp.train.BetaScheduler('quadratic', beta_max=args.beta)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWFyYFvY7DpW"
      },
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=STORAGE_PATH/'saved_models', save_best_only=True, save_weights_only=False, save_freq=1000\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fUe22V4lG7T",
        "outputId": "62971b79-09a8-40a0-986d-2064b0bff557"
      },
      "source": [
        "model.fit(train_dataset,\n",
        "          validation_data=valid_dataset,\n",
        "          epochs=1,\n",
        "          steps_per_epoch=10000,\n",
        "          callbacks=[BetaSchedulerCallback(beta_schedule.scheduler), checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step 00000: Beta is 0.0e+00.\n",
            "    1/10000 [..............................] - ETA: 253:24:39 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 162.1574 - beta: 0.0000e+00\n",
            "Step 00001: Beta is 1.0e-10.\n",
            "    2/10000 [..............................] - ETA: 4:52:47 - total_loss: 0.0028 - action_loss: 0.0028 - reg_loss: 161.9796 - beta: 0.0000e+00  \n",
            "Step 00002: Beta is 4.0e-10.\n",
            "    3/10000 [..............................] - ETA: 4:52:10 - total_loss: 0.0036 - action_loss: 0.0036 - reg_loss: 156.5387 - beta: 0.0000e+00\n",
            "Step 00003: Beta is 9.0e-10.\n",
            "    4/10000 [..............................] - ETA: 4:50:52 - total_loss: 0.0034 - action_loss: 0.0034 - reg_loss: 149.2674 - beta: 0.0000e+00\n",
            "Step 00004: Beta is 1.6e-09.\n",
            "    5/10000 [..............................] - ETA: 4:50:39 - total_loss: 0.0033 - action_loss: 0.0033 - reg_loss: 140.8841 - beta: 0.0000e+00\n",
            "Step 00005: Beta is 2.5e-09.\n",
            "    6/10000 [..............................] - ETA: 4:49:10 - total_loss: 0.0031 - action_loss: 0.0031 - reg_loss: 131.9084 - beta: 0.0000e+00WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 16.6498s). Check your callbacks.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0025s vs `on_train_batch_end` time: 16.6498s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step 00006: Beta is 3.6e-09.\n",
            "    7/10000 [..............................] - ETA: 4:49:16 - total_loss: 0.0030 - action_loss: 0.0030 - reg_loss: 123.0334 - beta: 0.0000e+00\n",
            "Step 00007: Beta is 4.9e-09.\n",
            "    8/10000 [..............................] - ETA: 4:49:22 - total_loss: 0.0030 - action_loss: 0.0030 - reg_loss: 114.7418 - beta: 0.0000e+00\n",
            "Step 00008: Beta is 6.4e-09.\n",
            "    9/10000 [..............................] - ETA: 4:51:30 - total_loss: 0.0029 - action_loss: 0.0029 - reg_loss: 107.4250 - beta: 0.0000e+00\n",
            "Step 00009: Beta is 8.1e-09.\n",
            "   10/10000 [..............................] - ETA: 4:51:19 - total_loss: 0.0029 - action_loss: 0.0029 - reg_loss: 101.1663 - beta: 0.0000e+00\n",
            "Step 00010: Beta is 1.0e-08.\n",
            "   11/10000 [..............................] - ETA: 4:52:11 - total_loss: 0.0028 - action_loss: 0.0028 - reg_loss: 95.7986 - beta: 0.0000e+00 \n",
            "Step 00011: Beta is 1.2e-08.\n",
            "   12/10000 [..............................] - ETA: 4:51:58 - total_loss: 0.0028 - action_loss: 0.0028 - reg_loss: 91.1776 - beta: 0.0000e+00\n",
            "Step 00012: Beta is 1.4e-08.\n",
            "   13/10000 [..............................] - ETA: 4:54:43 - total_loss: 0.0027 - action_loss: 0.0027 - reg_loss: 87.2703 - beta: 0.0000e+00\n",
            "Step 00013: Beta is 1.7e-08.\n",
            "   14/10000 [..............................] - ETA: 4:57:02 - total_loss: 0.0027 - action_loss: 0.0027 - reg_loss: 83.9589 - beta: 0.0000e+00\n",
            "Step 00014: Beta is 2.0e-08.\n",
            "   15/10000 [..............................] - ETA: 4:58:52 - total_loss: 0.0027 - action_loss: 0.0027 - reg_loss: 81.1250 - beta: 0.0000e+00\n",
            "Step 00015: Beta is 2.3e-08.\n",
            "   16/10000 [..............................] - ETA: 5:00:02 - total_loss: 0.0026 - action_loss: 0.0026 - reg_loss: 78.7371 - beta: 0.0000e+00\n",
            "Step 00016: Beta is 2.6e-08.\n",
            "   17/10000 [..............................] - ETA: 4:59:18 - total_loss: 0.0026 - action_loss: 0.0026 - reg_loss: 76.6830 - beta: 0.0000e+00\n",
            "Step 00017: Beta is 2.9e-08.\n",
            "   18/10000 [..............................] - ETA: 4:58:06 - total_loss: 0.0026 - action_loss: 0.0026 - reg_loss: 74.9482 - beta: 0.0000e+00\n",
            "Step 00018: Beta is 3.2e-08.\n",
            "   19/10000 [..............................] - ETA: 4:57:15 - total_loss: 0.0025 - action_loss: 0.0025 - reg_loss: 73.5144 - beta: 0.0000e+00\n",
            "Step 00019: Beta is 3.6e-08.\n",
            "   20/10000 [..............................] - ETA: 4:56:59 - total_loss: 0.0025 - action_loss: 0.0025 - reg_loss: 72.3276 - beta: 0.0000e+00\n",
            "Step 00020: Beta is 4.0e-08.\n",
            "   21/10000 [..............................] - ETA: 4:56:34 - total_loss: 0.0025 - action_loss: 0.0025 - reg_loss: 71.3693 - beta: 0.0000e+00\n",
            "Step 00021: Beta is 4.4e-08.\n",
            "   22/10000 [..............................] - ETA: 4:56:13 - total_loss: 0.0025 - action_loss: 0.0025 - reg_loss: 70.6256 - beta: 0.0000e+00\n",
            "Step 00022: Beta is 4.8e-08.\n",
            "   23/10000 [..............................] - ETA: 4:56:40 - total_loss: 0.0024 - action_loss: 0.0024 - reg_loss: 70.0436 - beta: 0.0000e+00\n",
            "Step 00023: Beta is 5.3e-08.\n",
            "   24/10000 [..............................] - ETA: 4:56:23 - total_loss: 0.0024 - action_loss: 0.0024 - reg_loss: 69.6332 - beta: 0.0000e+00\n",
            "Step 00024: Beta is 5.8e-08.\n",
            "   25/10000 [..............................] - ETA: 4:57:53 - total_loss: 0.0024 - action_loss: 0.0024 - reg_loss: 69.3661 - beta: 0.0000e+00\n",
            "Step 00025: Beta is 6.2e-08.\n",
            "   26/10000 [..............................] - ETA: 4:58:31 - total_loss: 0.0024 - action_loss: 0.0024 - reg_loss: 69.2256 - beta: 0.0000e+00\n",
            "Step 00026: Beta is 6.8e-08.\n",
            "   27/10000 [..............................] - ETA: 4:58:24 - total_loss: 0.0023 - action_loss: 0.0023 - reg_loss: 69.2037 - beta: 0.0000e+00\n",
            "Step 00027: Beta is 7.3e-08.\n",
            "   28/10000 [..............................] - ETA: 4:58:35 - total_loss: 0.0023 - action_loss: 0.0023 - reg_loss: 69.2729 - beta: 0.0000e+00\n",
            "Step 00028: Beta is 7.8e-08.\n",
            "   29/10000 [..............................] - ETA: 4:59:15 - total_loss: 0.0023 - action_loss: 0.0023 - reg_loss: 69.4603 - beta: 0.0000e+00\n",
            "Step 00029: Beta is 8.4e-08.\n",
            "   30/10000 [..............................] - ETA: 4:59:13 - total_loss: 0.0023 - action_loss: 0.0023 - reg_loss: 69.7366 - beta: 0.0000e+00\n",
            "Step 00030: Beta is 9.0e-08.\n",
            "   31/10000 [..............................] - ETA: 4:58:53 - total_loss: 0.0023 - action_loss: 0.0023 - reg_loss: 70.0963 - beta: 0.0000e+00\n",
            "Step 00031: Beta is 9.6e-08.\n",
            "   32/10000 [..............................] - ETA: 4:58:42 - total_loss: 0.0023 - action_loss: 0.0023 - reg_loss: 70.4940 - beta: 0.0000e+00\n",
            "Step 00032: Beta is 1.0e-07.\n",
            "   33/10000 [..............................] - ETA: 4:58:15 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 70.9801 - beta: 0.0000e+00\n",
            "Step 00033: Beta is 1.1e-07.\n",
            "   34/10000 [..............................] - ETA: 4:58:04 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 71.5253 - beta: 0.0000e+00\n",
            "Step 00034: Beta is 1.2e-07.\n",
            "   35/10000 [..............................] - ETA: 4:57:39 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 72.1311 - beta: 0.0000e+00\n",
            "Step 00035: Beta is 1.2e-07.\n",
            "   36/10000 [..............................] - ETA: 4:57:33 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 72.7602 - beta: 0.0000e+00\n",
            "Step 00036: Beta is 1.3e-07.\n",
            "   37/10000 [..............................] - ETA: 4:57:07 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 73.4322 - beta: 0.0000e+00\n",
            "Step 00037: Beta is 1.4e-07.\n",
            "   38/10000 [..............................] - ETA: 4:57:56 - total_loss: 0.0022 - action_loss: 0.0022 - reg_loss: 74.1464 - beta: 0.0000e+00\n",
            "Step 00038: Beta is 1.4e-07.\n",
            "   39/10000 [..............................] - ETA: 4:58:35 - total_loss: 0.0021 - action_loss: 0.0021 - reg_loss: 74.8983 - beta: 0.0000e+00\n",
            "Step 00039: Beta is 1.5e-07.\n",
            "   40/10000 [..............................] - ETA: 4:59:19 - total_loss: 0.0021 - action_loss: 0.0021 - reg_loss: 75.6876 - beta: 0.0000e+00\n",
            "Step 00040: Beta is 1.6e-07.\n",
            "   41/10000 [..............................] - ETA: 4:59:55 - total_loss: 0.0021 - action_loss: 0.0021 - reg_loss: 76.5215 - beta: 0.0000e+00\n",
            "Step 00041: Beta is 1.7e-07.\n",
            "   42/10000 [..............................] - ETA: 5:00:25 - total_loss: 0.0021 - action_loss: 0.0021 - reg_loss: 77.3810 - beta: 0.0000e+00\n",
            "Step 00042: Beta is 1.8e-07.\n",
            "   43/10000 [..............................] - ETA: 5:00:52 - total_loss: 0.0021 - action_loss: 0.0021 - reg_loss: 78.2692 - beta: 0.0000e+00\n",
            "Step 00043: Beta is 1.8e-07.\n",
            "   44/10000 [..............................] - ETA: 5:01:18 - total_loss: 0.0021 - action_loss: 0.0021 - reg_loss: 79.1855 - beta: 0.0000e+00\n",
            "Step 00044: Beta is 1.9e-07.\n",
            "   45/10000 [..............................] - ETA: 5:01:04 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 80.1419 - beta: 0.0000e+00\n",
            "Step 00045: Beta is 2.0e-07.\n",
            "   46/10000 [..............................] - ETA: 5:00:57 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 81.1234 - beta: 0.0000e+00\n",
            "Step 00046: Beta is 2.1e-07.\n",
            "   47/10000 [..............................] - ETA: 5:00:30 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 82.1336 - beta: 0.0000e+00\n",
            "Step 00047: Beta is 2.2e-07.\n",
            "   48/10000 [..............................] - ETA: 5:00:05 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 83.1825 - beta: 0.0000e+00\n",
            "Step 00048: Beta is 2.3e-07.\n",
            "   49/10000 [..............................] - ETA: 4:59:51 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 84.2459 - beta: 0.0000e+00\n",
            "Step 00049: Beta is 2.4e-07.\n",
            "   50/10000 [..............................] - ETA: 4:59:32 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 85.3332 - beta: 0.0000e+00\n",
            "Step 00050: Beta is 2.5e-07.\n",
            "   51/10000 [..............................] - ETA: 4:59:27 - total_loss: 0.0020 - action_loss: 0.0020 - reg_loss: 86.4301 - beta: 0.0000e+00\n",
            "Step 00051: Beta is 2.6e-07.\n",
            "   52/10000 [..............................] - ETA: 4:59:13 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 87.5370 - beta: 0.0000e+00\n",
            "Step 00052: Beta is 2.7e-07.\n",
            "   53/10000 [..............................] - ETA: 4:59:26 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 88.6829 - beta: 0.0000e+00\n",
            "Step 00053: Beta is 2.8e-07.\n",
            "   54/10000 [..............................] - ETA: 4:59:18 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 89.8411 - beta: 0.0000e+00\n",
            "Step 00054: Beta is 2.9e-07.\n",
            "   55/10000 [..............................] - ETA: 4:59:15 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 91.0175 - beta: 0.0000e+00\n",
            "Step 00055: Beta is 3.0e-07.\n",
            "   56/10000 [..............................] - ETA: 4:59:16 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 92.1902 - beta: 0.0000e+00\n",
            "Step 00056: Beta is 3.1e-07.\n",
            "   57/10000 [..............................] - ETA: 4:59:01 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 93.3638 - beta: 0.0000e+00\n",
            "Step 00057: Beta is 3.2e-07.\n",
            "   58/10000 [..............................] - ETA: 4:58:59 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 94.1749 - beta: 0.0000e+00\n",
            "Step 00058: Beta is 3.4e-07.\n",
            "   59/10000 [..............................] - ETA: 4:58:48 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 94.5259 - beta: 0.0000e+00\n",
            "Step 00059: Beta is 3.5e-07.\n",
            "   60/10000 [..............................] - ETA: 4:58:39 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 94.4258 - beta: 0.0000e+00\n",
            "Step 00060: Beta is 3.6e-07.\n",
            "   61/10000 [..............................] - ETA: 4:58:29 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 94.0201 - beta: 0.0000e+00\n",
            "Step 00061: Beta is 3.7e-07.\n",
            "   62/10000 [..............................] - ETA: 4:58:17 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 93.4013 - beta: 0.0000e+00\n",
            "Step 00062: Beta is 3.8e-07.\n",
            "   63/10000 [..............................] - ETA: 4:58:11 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 92.6477 - beta: 0.0000e+00\n",
            "Step 00063: Beta is 4.0e-07.\n",
            "   64/10000 [..............................] - ETA: 4:58:21 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 91.8139 - beta: 0.0000e+00\n",
            "Step 00064: Beta is 4.1e-07.\n",
            "   65/10000 [..............................] - ETA: 4:58:43 - total_loss: 0.0019 - action_loss: 0.0019 - reg_loss: 90.9525 - beta: 0.0000e+00\n",
            "Step 00065: Beta is 4.2e-07.\n",
            "   66/10000 [..............................] - ETA: 4:58:56 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 90.0862 - beta: 0.0000e+00\n",
            "Step 00066: Beta is 4.4e-07.\n",
            "   67/10000 [..............................] - ETA: 4:58:53 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 89.2387 - beta: 0.0000e+00\n",
            "Step 00067: Beta is 4.5e-07.\n",
            "   68/10000 [..............................] - ETA: 4:59:16 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 88.4181 - beta: 0.0000e+00\n",
            "Step 00068: Beta is 4.6e-07.\n",
            "   69/10000 [..............................] - ETA: 4:59:39 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 87.6312 - beta: 0.0000e+00\n",
            "Step 00069: Beta is 4.8e-07.\n",
            "   70/10000 [..............................] - ETA: 4:59:58 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 86.8808 - beta: 0.0000e+00\n",
            "Step 00070: Beta is 4.9e-07.\n",
            "   71/10000 [..............................] - ETA: 5:00:15 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 86.1720 - beta: 0.0000e+00\n",
            "Step 00071: Beta is 5.0e-07.\n",
            "   72/10000 [..............................] - ETA: 5:00:26 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 85.5036 - beta: 0.0000e+00\n",
            "Step 00072: Beta is 5.2e-07.\n",
            "   73/10000 [..............................] - ETA: 5:00:13 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 84.8783 - beta: 0.0000e+00\n",
            "Step 00073: Beta is 5.3e-07.\n",
            "   74/10000 [..............................] - ETA: 5:00:05 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 84.2974 - beta: 0.0000e+00\n",
            "Step 00074: Beta is 5.5e-07.\n",
            "   75/10000 [..............................] - ETA: 4:59:52 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 83.7577 - beta: 0.0000e+00\n",
            "Step 00075: Beta is 5.6e-07.\n",
            "   76/10000 [..............................] - ETA: 4:59:45 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 83.2611 - beta: 0.0000e+00\n",
            "Step 00076: Beta is 5.8e-07.\n",
            "   77/10000 [..............................] - ETA: 4:59:53 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 82.8112 - beta: 0.0000e+00\n",
            "Step 00077: Beta is 5.9e-07.\n",
            "   78/10000 [..............................] - ETA: 4:59:53 - total_loss: 0.0018 - action_loss: 0.0018 - reg_loss: 82.4062 - beta: 0.0000e+00\n",
            "Step 00078: Beta is 6.1e-07.\n",
            "   79/10000 [..............................] - ETA: 5:00:13 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 82.0428 - beta: 0.0000e+00\n",
            "Step 00079: Beta is 6.2e-07.\n",
            "   80/10000 [..............................] - ETA: 5:00:04 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 81.7197 - beta: 0.0000e+00\n",
            "Step 00080: Beta is 6.4e-07.\n",
            "   81/10000 [..............................] - ETA: 5:00:02 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 81.4379 - beta: 0.0000e+00\n",
            "Step 00081: Beta is 6.6e-07.\n",
            "   82/10000 [..............................] - ETA: 4:59:52 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 81.1886 - beta: 0.0000e+00\n",
            "Step 00082: Beta is 6.7e-07.\n",
            "   83/10000 [..............................] - ETA: 4:59:52 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.9785 - beta: 0.0000e+00\n",
            "Step 00083: Beta is 6.9e-07.\n",
            "   84/10000 [..............................] - ETA: 4:59:43 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.8012 - beta: 0.0000e+00\n",
            "Step 00084: Beta is 7.1e-07.\n",
            "   85/10000 [..............................] - ETA: 4:59:32 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.6562 - beta: 0.0000e+00\n",
            "Step 00085: Beta is 7.2e-07.\n",
            "   86/10000 [..............................] - ETA: 4:59:21 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.5408 - beta: 0.0000e+00\n",
            "Step 00086: Beta is 7.4e-07.\n",
            "   87/10000 [..............................] - ETA: 4:59:28 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.4480 - beta: 0.0000e+00\n",
            "Step 00087: Beta is 7.6e-07.\n",
            "   88/10000 [..............................] - ETA: 4:59:15 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.3866 - beta: 0.0000e+00\n",
            "Step 00088: Beta is 7.7e-07.\n",
            "   89/10000 [..............................] - ETA: 4:59:20 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.3483 - beta: 0.0000e+00\n",
            "Step 00089: Beta is 7.9e-07.\n",
            "   90/10000 [..............................] - ETA: 4:59:07 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.3272 - beta: 0.0000e+00\n",
            "Step 00090: Beta is 8.1e-07.\n",
            "   91/10000 [..............................] - ETA: 4:59:08 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.3318 - beta: 0.0000e+00\n",
            "Step 00091: Beta is 8.3e-07.\n",
            "   92/10000 [..............................] - ETA: 4:59:05 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.3558 - beta: 0.0000e+00\n",
            "Step 00092: Beta is 8.5e-07.\n",
            "   93/10000 [..............................] - ETA: 4:59:03 - total_loss: 0.0017 - action_loss: 0.0017 - reg_loss: 80.3931 - beta: 0.0000e+00\n",
            "Step 00093: Beta is 8.6e-07.\n",
            "   94/10000 [..............................] - ETA: 4:59:02 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 80.4471 - beta: 0.0000e+00\n",
            "Step 00094: Beta is 8.8e-07.\n",
            "   95/10000 [..............................] - ETA: 4:58:53 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 80.5202 - beta: 0.0000e+00\n",
            "Step 00095: Beta is 9.0e-07.\n",
            "   96/10000 [..............................] - ETA: 4:58:43 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 80.5991 - beta: 0.0000e+00\n",
            "Step 00096: Beta is 9.2e-07.\n",
            "   97/10000 [..............................] - ETA: 4:58:36 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 80.7013 - beta: 0.0000e+00\n",
            "Step 00097: Beta is 9.4e-07.\n",
            "   98/10000 [..............................] - ETA: 4:58:31 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 80.8228 - beta: 0.0000e+00\n",
            "Step 00098: Beta is 9.6e-07.\n",
            "   99/10000 [..............................] - ETA: 4:58:22 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 80.9537 - beta: 0.0000e+00\n",
            "Step 00099: Beta is 9.8e-07.\n",
            "  100/10000 [..............................] - ETA: 4:58:11 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 81.0939 - beta: 0.0000e+00\n",
            "Step 00100: Beta is 1.0e-06.\n",
            "  101/10000 [..............................] - ETA: 4:58:02 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 81.2546 - beta: 0.0000e+00\n",
            "Step 00101: Beta is 1.0e-06.\n",
            "  102/10000 [..............................] - ETA: 4:57:59 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 81.4286 - beta: 0.0000e+00\n",
            "Step 00102: Beta is 1.0e-06.\n",
            "  103/10000 [..............................] - ETA: 4:57:53 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 81.6149 - beta: 0.0000e+00\n",
            "Step 00103: Beta is 1.1e-06.\n",
            "  104/10000 [..............................] - ETA: 4:57:55 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 81.8187 - beta: 0.0000e+00\n",
            "Step 00104: Beta is 1.1e-06.\n",
            "  105/10000 [..............................] - ETA: 4:57:55 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 82.0371 - beta: 0.0000e+00\n",
            "Step 00105: Beta is 1.1e-06.\n",
            "  106/10000 [..............................] - ETA: 4:57:47 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 82.2747 - beta: 0.0000e+00\n",
            "Step 00106: Beta is 1.1e-06.\n",
            "  107/10000 [..............................] - ETA: 4:57:48 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 82.5293 - beta: 0.0000e+00\n",
            "Step 00107: Beta is 1.1e-06.\n",
            "  108/10000 [..............................] - ETA: 4:57:45 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 82.8086 - beta: 0.0000e+00\n",
            "Step 00108: Beta is 1.2e-06.\n",
            "  109/10000 [..............................] - ETA: 4:57:38 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 83.1014 - beta: 0.0000e+00\n",
            "Step 00109: Beta is 1.2e-06.\n",
            "  110/10000 [..............................] - ETA: 4:57:29 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 83.4096 - beta: 0.0000e+00\n",
            "Step 00110: Beta is 1.2e-06.\n",
            "  111/10000 [..............................] - ETA: 4:57:22 - total_loss: 0.0016 - action_loss: 0.0016 - reg_loss: 83.7350 - beta: 0.0000e+00\n",
            "Step 00111: Beta is 1.2e-06.\n",
            "  112/10000 [..............................] - ETA: 4:57:19 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.0838 - beta: 0.0000e+00\n",
            "Step 00112: Beta is 1.3e-06.\n",
            "  113/10000 [..............................] - ETA: 4:57:17 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.4495 - beta: 0.0000e+00\n",
            "Step 00113: Beta is 1.3e-06.\n",
            "  114/10000 [..............................] - ETA: 4:57:11 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.8274 - beta: 0.0000e+00\n",
            "Step 00114: Beta is 1.3e-06.\n",
            "  115/10000 [..............................] - ETA: 4:57:04 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.2241 - beta: 0.0000e+00\n",
            "Step 00115: Beta is 1.3e-06.\n",
            "  116/10000 [..............................] - ETA: 4:57:00 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.6504 - beta: 0.0000e+00\n",
            "Step 00116: Beta is 1.3e-06.\n",
            "  117/10000 [..............................] - ETA: 4:56:58 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.0880 - beta: 0.0000e+00\n",
            "Step 00117: Beta is 1.4e-06.\n",
            "  118/10000 [..............................] - ETA: 4:56:50 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.5472 - beta: 0.0000e+00\n",
            "Step 00118: Beta is 1.4e-06.\n",
            "  119/10000 [..............................] - ETA: 4:57:02 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.0166 - beta: 0.0000e+00\n",
            "Step 00119: Beta is 1.4e-06.\n",
            "  120/10000 [..............................] - ETA: 4:56:53 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.4991 - beta: 0.0000e+00\n",
            "Step 00120: Beta is 1.4e-06.\n",
            "  121/10000 [..............................] - ETA: 4:56:50 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.9745 - beta: 0.0000e+00\n",
            "Step 00121: Beta is 1.5e-06.\n",
            "  122/10000 [..............................] - ETA: 4:56:40 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.4077 - beta: 0.0000e+00\n",
            "Step 00122: Beta is 1.5e-06.\n",
            "  123/10000 [..............................] - ETA: 4:56:49 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.6466 - beta: 0.0000e+00\n",
            "Step 00123: Beta is 1.5e-06.\n",
            "  124/10000 [..............................] - ETA: 4:56:46 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.7624 - beta: 0.0000e+00\n",
            "Step 00124: Beta is 1.5e-06.\n",
            "  125/10000 [..............................] - ETA: 4:56:41 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.8207 - beta: 0.0000e+00\n",
            "Step 00125: Beta is 1.6e-06.\n",
            "  126/10000 [..............................] - ETA: 4:56:38 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.7650 - beta: 0.0000e+00\n",
            "Step 00126: Beta is 1.6e-06.\n",
            "  127/10000 [..............................] - ETA: 4:56:34 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.6308 - beta: 0.0000e+00\n",
            "Step 00127: Beta is 1.6e-06.\n",
            "  128/10000 [..............................] - ETA: 4:56:35 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.4439 - beta: 0.0000e+00\n",
            "Step 00128: Beta is 1.6e-06.\n",
            "  129/10000 [..............................] - ETA: 4:56:29 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.2202 - beta: 0.0000e+00\n",
            "Step 00129: Beta is 1.7e-06.\n",
            "  130/10000 [..............................] - ETA: 4:56:25 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.9736 - beta: 0.0000e+00\n",
            "Step 00130: Beta is 1.7e-06.\n",
            "  131/10000 [..............................] - ETA: 4:56:31 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.7149 - beta: 0.0000e+00\n",
            "Step 00131: Beta is 1.7e-06.\n",
            "  132/10000 [..............................] - ETA: 4:56:31 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.4460 - beta: 0.0000e+00\n",
            "Step 00132: Beta is 1.7e-06.\n",
            "  133/10000 [..............................] - ETA: 4:56:32 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.1732 - beta: 0.0000e+00\n",
            "Step 00133: Beta is 1.8e-06.\n",
            "  134/10000 [..............................] - ETA: 4:56:32 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.9026 - beta: 0.0000e+00\n",
            "Step 00134: Beta is 1.8e-06.\n",
            "  135/10000 [..............................] - ETA: 4:56:24 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.6343 - beta: 0.0000e+00\n",
            "Step 00135: Beta is 1.8e-06.\n",
            "  136/10000 [..............................] - ETA: 4:56:24 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.3743 - beta: 0.0000e+00\n",
            "Step 00136: Beta is 1.8e-06.\n",
            "  137/10000 [..............................] - ETA: 4:56:14 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.1271 - beta: 0.0000e+00\n",
            "Step 00137: Beta is 1.9e-06.\n",
            "  138/10000 [..............................] - ETA: 4:56:12 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.8925 - beta: 0.0000e+00\n",
            "Step 00138: Beta is 1.9e-06.\n",
            "  139/10000 [..............................] - ETA: 4:56:12 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.6692 - beta: 0.0000e+00\n",
            "Step 00139: Beta is 1.9e-06.\n",
            "  140/10000 [..............................] - ETA: 4:56:18 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.4576 - beta: 0.0000e+00\n",
            "Step 00140: Beta is 2.0e-06.\n",
            "  141/10000 [..............................] - ETA: 4:56:14 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.2622 - beta: 0.0000e+00\n",
            "Step 00141: Beta is 2.0e-06.\n",
            "  142/10000 [..............................] - ETA: 4:56:12 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.0840 - beta: 0.0000e+00\n",
            "Step 00142: Beta is 2.0e-06.\n",
            "  143/10000 [..............................] - ETA: 4:56:13 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.9235 - beta: 0.0000e+00\n",
            "Step 00143: Beta is 2.0e-06.\n",
            "  144/10000 [..............................] - ETA: 4:56:18 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.7827 - beta: 0.0000e+00\n",
            "Step 00144: Beta is 2.1e-06.\n",
            "  145/10000 [..............................] - ETA: 4:56:14 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.6586 - beta: 0.0000e+00\n",
            "Step 00145: Beta is 2.1e-06.\n",
            "  146/10000 [..............................] - ETA: 4:56:22 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.5488 - beta: 0.0000e+00\n",
            "Step 00146: Beta is 2.1e-06.\n",
            "  147/10000 [..............................] - ETA: 4:56:13 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.4561 - beta: 0.0000e+00\n",
            "Step 00147: Beta is 2.2e-06.\n",
            "  148/10000 [..............................] - ETA: 4:56:11 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.3808 - beta: 0.0000e+00\n",
            "Step 00148: Beta is 2.2e-06.\n",
            "  149/10000 [..............................] - ETA: 4:56:03 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.3206 - beta: 0.0000e+00\n",
            "Step 00149: Beta is 2.2e-06.\n",
            "  150/10000 [..............................] - ETA: 4:55:57 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2752 - beta: 0.0000e+00\n",
            "Step 00150: Beta is 2.3e-06.\n",
            "  151/10000 [..............................] - ETA: 4:55:52 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2419 - beta: 0.0000e+00\n",
            "Step 00151: Beta is 2.3e-06.\n",
            "  152/10000 [..............................] - ETA: 4:55:49 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2205 - beta: 0.0000e+00\n",
            "Step 00152: Beta is 2.3e-06.\n",
            "  153/10000 [..............................] - ETA: 4:55:51 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2132 - beta: 0.0000e+00\n",
            "Step 00153: Beta is 2.3e-06.\n",
            "  154/10000 [..............................] - ETA: 4:55:48 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2190 - beta: 0.0000e+00\n",
            "Step 00154: Beta is 2.4e-06.\n",
            "  155/10000 [..............................] - ETA: 4:55:48 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2273 - beta: 0.0000e+00\n",
            "Step 00155: Beta is 2.4e-06.\n",
            "  156/10000 [..............................] - ETA: 4:55:47 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2463 - beta: 0.0000e+00\n",
            "Step 00156: Beta is 2.4e-06.\n",
            "  157/10000 [..............................] - ETA: 4:55:46 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.2782 - beta: 0.0000e+00\n",
            "Step 00157: Beta is 2.5e-06.\n",
            "  158/10000 [..............................] - ETA: 4:55:52 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.3200 - beta: 0.0000e+00\n",
            "Step 00158: Beta is 2.5e-06.\n",
            "  159/10000 [..............................] - ETA: 4:56:04 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.3726 - beta: 0.0000e+00\n",
            "Step 00159: Beta is 2.5e-06.\n",
            "  160/10000 [..............................] - ETA: 4:56:07 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 84.4385 - beta: 0.0000e+00\n",
            "Step 00160: Beta is 2.6e-06.\n",
            "  161/10000 [..............................] - ETA: 4:56:17 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.5147 - beta: 0.0000e+00\n",
            "Step 00161: Beta is 2.6e-06.\n",
            "  162/10000 [..............................] - ETA: 4:56:24 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.5979 - beta: 0.0000e+00\n",
            "Step 00162: Beta is 2.6e-06.\n",
            "  163/10000 [..............................] - ETA: 4:56:31 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.6920 - beta: 0.0000e+00\n",
            "Step 00163: Beta is 2.7e-06.\n",
            "  164/10000 [..............................] - ETA: 4:56:37 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.7956 - beta: 0.0000e+00\n",
            "Step 00164: Beta is 2.7e-06.\n",
            "  165/10000 [..............................] - ETA: 4:56:47 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 84.9103 - beta: 0.0000e+00\n",
            "Step 00165: Beta is 2.7e-06.\n",
            "  166/10000 [..............................] - ETA: 4:56:56 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.0345 - beta: 0.0000e+00\n",
            "Step 00166: Beta is 2.8e-06.\n",
            "  167/10000 [..............................] - ETA: 4:56:49 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.1669 - beta: 0.0000e+00\n",
            "Step 00167: Beta is 2.8e-06.\n",
            "  168/10000 [..............................] - ETA: 4:56:42 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.3074 - beta: 0.0000e+00\n",
            "Step 00168: Beta is 2.8e-06.\n",
            "  169/10000 [..............................] - ETA: 4:56:43 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.4547 - beta: 0.0000e+00\n",
            "Step 00169: Beta is 2.9e-06.\n",
            "  170/10000 [..............................] - ETA: 4:56:37 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.6117 - beta: 0.0000e+00\n",
            "Step 00170: Beta is 2.9e-06.\n",
            "  171/10000 [..............................] - ETA: 4:56:37 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.7784 - beta: 0.0000e+00\n",
            "Step 00171: Beta is 2.9e-06.\n",
            "  172/10000 [..............................] - ETA: 4:56:33 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 85.9515 - beta: 0.0000e+00\n",
            "Step 00172: Beta is 3.0e-06.\n",
            "  173/10000 [..............................] - ETA: 4:56:27 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.1311 - beta: 0.0000e+00\n",
            "Step 00173: Beta is 3.0e-06.\n",
            "  174/10000 [..............................] - ETA: 4:56:29 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.3164 - beta: 0.0000e+00\n",
            "Step 00174: Beta is 3.0e-06.\n",
            "  175/10000 [..............................] - ETA: 4:56:31 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.5096 - beta: 0.0000e+00\n",
            "Step 00175: Beta is 3.1e-06.\n",
            "  176/10000 [..............................] - ETA: 4:56:37 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.7114 - beta: 0.0000e+00\n",
            "Step 00176: Beta is 3.1e-06.\n",
            "  177/10000 [..............................] - ETA: 4:56:39 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 86.9176 - beta: 0.0000e+00\n",
            "Step 00177: Beta is 3.1e-06.\n",
            "  178/10000 [..............................] - ETA: 4:56:36 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.1317 - beta: 0.0000e+00\n",
            "Step 00178: Beta is 3.2e-06.\n",
            "  179/10000 [..............................] - ETA: 4:56:34 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.3536 - beta: 0.0000e+00\n",
            "Step 00179: Beta is 3.2e-06.\n",
            "  180/10000 [..............................] - ETA: 4:56:32 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.5875 - beta: 0.0000e+00\n",
            "Step 00180: Beta is 3.2e-06.\n",
            "  181/10000 [..............................] - ETA: 4:56:36 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 87.8313 - beta: 0.0000e+00\n",
            "Step 00181: Beta is 3.3e-06.\n",
            "  182/10000 [..............................] - ETA: 4:56:44 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.0813 - beta: 0.0000e+00\n",
            "Step 00182: Beta is 3.3e-06.\n",
            "  183/10000 [..............................] - ETA: 4:56:48 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.3413 - beta: 0.0000e+00\n",
            "Step 00183: Beta is 3.3e-06.\n",
            "  184/10000 [..............................] - ETA: 4:56:42 - total_loss: 0.0015 - action_loss: 0.0015 - reg_loss: 88.6045 - beta: 0.0000e+00\n",
            "Step 00184: Beta is 3.4e-06.\n",
            "  185/10000 [..............................] - ETA: 4:56:43 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 88.8731 - beta: 0.0000e+00\n",
            "Step 00185: Beta is 3.4e-06.\n",
            "  186/10000 [..............................] - ETA: 4:56:40 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 89.1471 - beta: 0.0000e+00\n",
            "Step 00186: Beta is 3.5e-06.\n",
            "  187/10000 [..............................] - ETA: 4:56:44 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 89.4244 - beta: 0.0000e+00\n",
            "Step 00187: Beta is 3.5e-06.\n",
            "  188/10000 [..............................] - ETA: 4:56:37 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 89.7057 - beta: 0.0000e+00\n",
            "Step 00188: Beta is 3.5e-06.\n",
            "  189/10000 [..............................] - ETA: 4:56:32 - total_loss: 0.0014 - action_loss: 0.0014 - reg_loss: 89.9846 - beta: 0.0000e+00\n",
            "Step 00189: Beta is 3.6e-06.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw21h14BnX3Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}