{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "LMP Test.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "G84fGXVwv9Fh",
        "-yC0L1P3v9GA",
        "8SHQGBjnv9GK",
        "b7CMny5dv9Gx",
        "njbQpQAGv9G2"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sholtodouglas/learning_from_play/blob/master/notebooks/train_lfp_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jUYgZwpGfav"
      },
      "source": [
        "!pip install -q wandb pathy comet_ml"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFJvewo0ee6r",
        "outputId": "88277821-f2da-411d-963e-659d1a8d3b57"
      },
      "source": [
        "# import comet_ml at the top of your file\n",
        "from comet_ml import Experiment\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msholto\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z68MkXdDZaSZ",
        "outputId": "7249f203-22f3-4135-9680-8a9a06d80930"
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='LFP training arguments')\n",
        "parser.add_argument('run_name')\n",
        "parser.add_argument('--train_datasets', nargs='+', help='Training dataset names')\n",
        "parser.add_argument('--test_datasets', nargs='+', help='Testing dataset names')\n",
        "parser.add_argument('-c', '--colab', default=False, action='store_true', help='Enable if using colab environment')\n",
        "parser.add_argument('-s', '--data_source', default='DRIVE', help='Source of training data')\n",
        "parser.add_argument('-tfr', '--from_tfrecords', default=False, action='store_true', help='Enable if using tfrecords format')\n",
        "parser.add_argument('-d', '--device', default='TPU', help='Hardware device to train on')\n",
        "parser.add_argument('-b', '--batch_size', default=512, type=int)\n",
        "parser.add_argument('-wmax', '--window_size_max', default=50, type=int)\n",
        "parser.add_argument('-wmin', '--window_size_min', default=20, type=int)\n",
        "parser.add_argument('-la', '--actor_layer_size', default=2048, type=int, help='Layer size of actor, increases size of neural net')\n",
        "parser.add_argument('-le', '--encoder_layer_size', default=512, type=int, help='Layer size of encoder, increases size of neural net')\n",
        "parser.add_argument('-lp', '--planner_layer_size', default=512, type=int, help='Layer size of planner, increases size of neural net')\n",
        "parser.add_argument('-embd', '--img_embedding_size', default=64, type=int, help='Embedding size of features,goal space')\n",
        "parser.add_argument('-z', '--latent_dim', default=256, type=int, help='Size of the VAE latent space')\n",
        "parser.add_argument('-g', '--gcbc', default=False, action='store_true', help='Enables GCBC, a simpler model with no encoder/planner')\n",
        "parser.add_argument('-n', '--num_distribs', default=None, type=int, help='Number of distributions to use in logistic mixture model')\n",
        "parser.add_argument('-q', '--qbits', default=None, type=int, help='Number of quantisation bits to discrete distributions into. Total quantisations = 2**qbits')\n",
        "parser.add_argument('-lr', '--learning_rate', type=float, default=3e-4)\n",
        "parser.add_argument('-t', '--train_steps', type=int, default=200000)\n",
        "parser.add_argument('-r', '--resume', default=False, action='store_true')\n",
        "parser.add_argument('-B', '--beta', type=float, default=0.00003)\n",
        "parser.add_argument('-i', '--images', default=False, action='store_true')\n",
        "parser.add_argument('--fp16', default=False, action='store_true')\n",
        "parser.add_argument('--bucket_name', help='GCS bucket name to stream data from')\n",
        "parser.add_argument('--tpu_name', help='GCP TPU name') # Only used in the script on GCP\n",
        "\n",
        "\n",
        "# ## Sample colab config\n",
        "# args = parser.parse_args('''\n",
        "# refactor_test\n",
        "# --train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
        "# --test_dataset UR5_slow_gripper_test\n",
        "# -c\n",
        "# -tfr\n",
        "# -s GCS\n",
        "# -d TPU\n",
        "# -b 512\n",
        "# -la 2048\n",
        "# -le 512\n",
        "# -lp 512\n",
        "# -z 256\n",
        "# -lr 3e-4\n",
        "# --bucket_name iowa_bucket_lfp\n",
        "# -i\n",
        "# '''.split())\n",
        "\n",
        "## Sample colab config\n",
        "args = parser.parse_args('''\n",
        "fit_test\n",
        "--train_dataset UR5 UR5_slow_gripper UR5_high_transition\n",
        "--test_dataset UR5_slow_gripper_test\n",
        "-c\n",
        "-s DRIVE\n",
        "-d TPU\n",
        "-b 512\n",
        "-la 2048\n",
        "-le 512\n",
        "-lp 512\n",
        "-z 256\n",
        "-lr 3e-4\n",
        "-B 0.00003\n",
        "'''.split())\n",
        "\n",
        "# -n 5\n",
        "# -q 8\n",
        "\n",
        "print(args)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(actor_layer_size=2048, batch_size=512, beta=3e-05, bucket_name=None, colab=True, data_source='DRIVE', device='TPU', encoder_layer_size=512, fp16=False, from_tfrecords=False, gcbc=False, images=False, img_embedding_size=64, latent_dim=256, learning_rate=0.0003, num_distribs=None, planner_layer_size=512, qbits=None, resume=False, run_name='fit_test', test_datasets=['UR5_slow_gripper_test'], tpu_name=None, train_datasets=['UR5', 'UR5_slow_gripper', 'UR5_high_transition'], train_steps=200000, window_size_max=50, window_size_min=20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EliqxOpPv9Dy"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMJ___6ARZ4u",
        "outputId": "f11188e9-cd67-4425-a001-35f32eb31b4b"
      },
      "source": [
        "from pathlib import Path\n",
        "from pathy import Pathy\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "import pprint\n",
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "#@title Workpace Setup (Local vs Colab)\n",
        "\n",
        "# Set up working directory and libraries\n",
        "if args.colab:\n",
        "    from google.colab import drive, auth\n",
        "    print('Using colab setup')\n",
        "    WORKING_PATH = Path('/content/learning_from_play')\n",
        "    # Clone repo\n",
        "    try:\n",
        "        get_ipython().system(\"git clone 'https://github.com/sholtodouglas/learning_from_play' {WORKING_PATH}\")\n",
        "    except: \n",
        "        pass\n",
        "    # Mount drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print('Using local setup')\n",
        "    WORKING_PATH = Path.cwd()\n",
        "    print(f'Working path: {WORKING_PATH}')\n",
        "\n",
        "# Change working directory to learning_from_play\n",
        "os.chdir(WORKING_PATH)\n",
        "import lfp\n",
        "\n",
        "# Set up storage directory and datasets\n",
        "if args.data_source == 'DRIVE':\n",
        "    assert args.colab, \"Must be using Colab\"\n",
        "    print('Reading data from Google Drive')\n",
        "    STORAGE_PATH = Path('/content/drive/My Drive/Robotic Learning')\n",
        "elif args.data_source == 'GCS':\n",
        "    if args.colab:\n",
        "      auth.authenticate_user()\n",
        "    print('Reading data from Google Cloud Storage')\n",
        "    r = requests.get('https://ipinfo.io')\n",
        "    region = r.json()['region']\n",
        "    project_id = 'learning-from-play-303306'\n",
        "    logging.warning(f'You are accessing GCS data from {region}, make sure this is the same as your bucket {args.bucket_name}')\n",
        "    STORAGE_PATH = Pathy(f'gs://{args.bucket_name}')\n",
        "else:\n",
        "    print('Reading data from local filesystem')\n",
        "    STORAGE_PATH = WORKING_PATH\n",
        "\n",
        "print(f'Storage path: {STORAGE_PATH}')\n",
        "TRAIN_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.train_datasets]\n",
        "TEST_DATA_PATHS = [STORAGE_PATH/'data'/x for x in args.test_datasets]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using colab setup\n",
            "fatal: destination path '/content/learning_from_play' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "No pybullet installation found - which is fine if training\n",
            "Reading data from Google Drive\n",
            "Storage path: /content/drive/My Drive/Robotic Learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zbmq324x2yv",
        "outputId": "fb9d7b98-efa2-4034-c802-300d5ff13419"
      },
      "source": [
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "if args.device == 'TPU':\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=args.tpu_name)  # TPU detection\n",
        "        print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    except ValueError:\n",
        "        raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    NUM_DEVICES = strategy.num_replicas_in_sync\n",
        "    print(\"REPLICAS: \", NUM_DEVICES)\n",
        "    if args.fp16:\n",
        "        tf.keras.mixed_precision.set_global_policy('mixed_bfloat16')\n",
        "else:\n",
        "    physical_devices = tf.config.list_physical_devices()\n",
        "    if args.device == 'GPU':\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[3], enable=True)\n",
        "        if args.fp16:\n",
        "            tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    NUM_DEVICES = 1\n",
        "    print(physical_devices)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.1\n",
            "Running on TPU  ['10.49.81.162:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.49.81.162:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.49.81.162:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HejtDH_Yx8h",
        "outputId": "90ddd169-4a83-445f-db02-2fd9ab526d34"
      },
      "source": [
        "# Use this to edit modules without needing to restart the kernel (can also edit local, push/pull)\n",
        "!git pull\n",
        "import importlib\n",
        "importlib.reload(lfp.data)\n",
        "importlib.reload(lfp.model)\n",
        "importlib.reload(lfp.train)\n",
        "importlib.reload(lfp.metric)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'lfp.metric' from '/content/learning_from_play/lfp/metric.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUI6JP06FdTv"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2lBWIyOFdTv"
      },
      "source": [
        "GLOBAL_BATCH_SIZE = args.batch_size * NUM_DEVICES\n",
        "dl = lfp.data.PlayDataloader(include_imgs = args.images, batch_size=GLOBAL_BATCH_SIZE,  window_size=args.window_size_max, min_window_size=args.window_size_min)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uiz4AXCnFdTv",
        "outputId": "861fb597-ac1f-4863-95f3-d9d1dcf2112b"
      },
      "source": [
        "# Train data\n",
        "train_data = dl.extract(TRAIN_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
        "train_dataset = dl.load(train_data)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UR5: 100%|██████████| 52/52 [00:01<00:00, 50.27it/s]\n",
            "UR5_slow_gripper: 100%|██████████| 23/23 [00:00<00:00, 47.20it/s]\n",
            "UR5_high_transition: 100%|██████████| 32/32 [00:00<00:00, 54.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{   'acts': TensorSpec(shape=(4096, 50, 7), dtype=tf.float32, name=None),\n",
            "    'dataset_path': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None),\n",
            "    'goals': TensorSpec(shape=(4096, 50, 11), dtype=tf.float32, name=None),\n",
            "    'masks': TensorSpec(shape=(4096, 50), dtype=tf.float32, name=None),\n",
            "    'obs': TensorSpec(shape=(4096, 50, 18), dtype=tf.float32, name=None),\n",
            "    'seq_lens': TensorSpec(shape=(4096,), dtype=tf.float32, name=None),\n",
            "    'tstep_idxs': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ZDAVpLFdTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4350e1-089d-4a70-d68e-8e84ba0f5fe3"
      },
      "source": [
        "# Validation data\n",
        "valid_data = dl.extract(TEST_DATA_PATHS, from_tfrecords=args.from_tfrecords)\n",
        "valid_dataset = dl.load(valid_data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UR5_slow_gripper_test: 100%|██████████| 2/2 [00:00<00:00, 38.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{   'acts': TensorSpec(shape=(4096, 50, 7), dtype=tf.float32, name=None),\n",
            "    'dataset_path': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None),\n",
            "    'goals': TensorSpec(shape=(4096, 50, 11), dtype=tf.float32, name=None),\n",
            "    'masks': TensorSpec(shape=(4096, 50), dtype=tf.float32, name=None),\n",
            "    'obs': TensorSpec(shape=(4096, 50, 18), dtype=tf.float32, name=None),\n",
            "    'seq_lens': TensorSpec(shape=(4096,), dtype=tf.float32, name=None),\n",
            "    'tstep_idxs': TensorSpec(shape=(4096, None, 1), dtype=tf.int32, name=None)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWIpRPRuv9E6",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmVwdAknhGrS"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Input, LSTM, Concatenate, Masking, Reshape, Lambda, \\\n",
        "    Bidirectional, GRU, LayerNormalization, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.metrics import Mean\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tfpl = tfp.layers\n",
        "\n",
        "ACT_LIMITS = tf.constant([1.5, 1.5, 2.2, 3.2, 3.2, 3.2, 1.1])\n",
        "\n",
        "def create_actor(obs_dim, act_dim, goal_dim, layer_size=1024, latent_dim=256, training=True):\n",
        "    # params #\n",
        "    batch_size = None if training else 1\n",
        "    stateful = not training\n",
        "\n",
        "    # Input #\n",
        "    o = Input(shape=(None, obs_dim), batch_size=batch_size, dtype=tf.float32, name='input_obs')\n",
        "    z = Input(shape=(None, latent_dim), batch_size=batch_size, dtype=tf.float32, name='input_latent')\n",
        "    g = Input(shape=(None, goal_dim), batch_size=batch_size, dtype=tf.float32, name='input_goals')\n",
        "\n",
        "    # RNN #\n",
        "    x = Concatenate(axis=-1)([o, z, g])\n",
        "    x = Masking(mask_value=0.)(x)\n",
        "    x = LSTM(layer_size, return_sequences=True, stateful=stateful, name='LSTM_in_1')(x)\n",
        "    x = LSTM(layer_size, return_sequences=True, stateful=stateful, name='LSTM_in_2')(x)\n",
        "\n",
        "    # Deterministic output #\n",
        "    actions = Dense(act_dim, activation='tanh', name='acts')(x)\n",
        "    actions = Lambda(lambda a: a * ACT_LIMITS)(actions) # scale to action limits\n",
        "    return Model([o, z, g], actions)\n",
        "\n",
        "\n",
        "def create_encoder(obs_dim, act_dim, layer_size=2048, latent_dim=256):\n",
        "    # Input #\n",
        "    obs = Input(shape=(None, obs_dim), dtype=tf.float32, name='obs')\n",
        "    acts = Input(shape=(None, act_dim), dtype=tf.float32, name='acts')\n",
        "\n",
        "    # Layers #\n",
        "    x = Concatenate(axis=-1)([obs, acts])\n",
        "    x = Masking(mask_value=0.)(x)\n",
        "    x = Bidirectional(LSTM(layer_size, return_sequences=True), merge_mode='concat')(x)\n",
        "    x = Bidirectional(LSTM(layer_size, return_sequences=False), merge_mode='concat')(x)\n",
        "\n",
        "    # Latent Variable #\n",
        "    x = Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None)(x),\n",
        "    z = tfpl.MultivariateNormalTriL(latent_dim, name='latent')(x)\n",
        "    return Model([obs, acts], z)\n",
        "\n",
        "\n",
        "def create_planner(obs_dim, goal_dim, layer_size=2048, latent_dim=256):\n",
        "    # params #\n",
        "    batch_size = None\n",
        "\n",
        "    # Input #\n",
        "    o_i = Input(shape=(obs_dim,), batch_size=batch_size, dtype=tf.float32,\n",
        "                name='initial_obs')  # has arm state\n",
        "    o_g = Input(shape=(goal_dim,), batch_size=batch_size, dtype=tf.float32,\n",
        "                name='goal_obs')  # does not have arm state\n",
        "\n",
        "    # Layers #\n",
        "    x = Concatenate(axis=-1)([o_i, o_g])\n",
        "    x = Masking(mask_value=0.)(x)\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_1')(x) # maybe change to selu/gelu/swish?\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_2')(x)\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_3')(x)\n",
        "    x = Dense(layer_size, activation=\"relu\", name='layer_4')(x)\n",
        "\n",
        "    # Latent Variable #\n",
        "    x = Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None)(x),\n",
        "    z = tfpl.MultivariateNormalTriL(latent_dim, name='latent')(x)\n",
        "    return Model([o_i, o_g], z)\n",
        "\n",
        "# Todo: add beta callback, add checkpointing callback, think about train=False autoregressive, what to do about masking?\n",
        "# Account for probabilistic (need to sample the actions to get MAE)\n",
        "class LFPNet(Model):\n",
        "    def __init__(self, encoder, planner, actor, beta) -> None:\n",
        "        super(LFPNet, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.planner = planner\n",
        "        self.actor = actor\n",
        "        self.beta = beta\n",
        "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "        self.valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
        "\n",
        "        self.train_act_with_enc_loss = tf.keras.metrics.Mean(name='train_act_with_enc_loss')\n",
        "        self.train_act_with_plan_loss = tf.keras.metrics.Mean(name='train_act_with_plan_loss')\n",
        "        self.valid_act_with_enc_loss = tf.keras.metrics.Mean(name='valid_act_with_enc_loss')\n",
        "        self.valid_act_with_plan_loss = tf.keras.metrics.Mean(name='valid_act_with_plan_loss')\n",
        "\n",
        "        self.train_reg_loss = tf.keras.metrics.Mean(name='reg_loss')\n",
        "        self.valid_reg_loss = tf.keras.metrics.Mean(name='valid_reg_loss')\n",
        "\n",
        "        self.valid_position_loss = tf.keras.metrics.Mean(name='valid_position_loss')\n",
        "        self.valid_max_position_loss = lfp.metric.MaxMetric(name='valid_max_position_loss')\n",
        "        self.valid_rotation_loss = tf.keras.metrics.Mean(name='valid_rotation_loss')\n",
        "        self.valid_max_rotation_loss = lfp.metric.MaxMetric(name='valid_max_rotation_loss')\n",
        "        self.valid_gripper_loss = tf.keras.metrics.Mean(name='valid_rotation_loss')\n",
        "\n",
        "    def call(self, inputs, planner=True, training=False):\n",
        "        if planner:\n",
        "            z = self.planner([inputs['obs'][:,0,:], inputs['goals'][:,0,:]])\n",
        "        else:\n",
        "            z = self.encoder([inputs['obs'], inputs['acts']])\n",
        "        z_tiled = tf.tile(tf.expand_dims(z[0], 1), (1, inputs['obs'].shape[1], 1))\n",
        "        acts = self.actor([inputs['obs'], z_tiled, inputs['goals']])\n",
        "        return acts, z\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        with tf.GradientTape() as tape:\n",
        "            acts_enc, z_enc = self(inputs, planner=False, training=True)\n",
        "            acts_plan, z_plan = self(inputs, planner=True, training=True)\n",
        "            act_enc_loss = self.compiled_loss(inputs['acts'], acts_enc, regularization_losses=self.losses)\n",
        "            act_plan_loss = self.compiled_loss(inputs['acts'], acts_plan, regularization_losses=self.losses)\n",
        "\n",
        "            reg_loss = tfd.kl_divergence(z_enc, z_plan)\n",
        "            loss = act_enc_loss + self.beta * reg_loss\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        \n",
        "        # Update metrics \n",
        "        self.train_loss.update_state(loss)\n",
        "        self.train_act_with_enc_loss.update_state(act_enc_loss)\n",
        "        self.train_act_with_plan_loss.update_state(act_plan_loss)\n",
        "        self.train_reg_loss.update_state(reg_loss)\n",
        "\n",
        "        result = {m.name: m.result() for m in self.metrics}\n",
        "        result['beta'] = self.beta\n",
        "        return result\n",
        "\n",
        "    def test_step(self, inputs):\n",
        "        acts_enc, z_enc = self(inputs, planner=False, training=False)\n",
        "        acts_plan, z_plan = self(inputs, planner=True, training=False)\n",
        "        act_enc_loss = self.compiled_loss(inputs['acts'], acts_enc, regularization_losses=self.losses)\n",
        "        act_plan_loss = self.compiled_loss(inputs['acts'], acts_plan, regularization_losses=self.losses)\n",
        "\n",
        "        reg_loss = tfd.kl_divergence(z_enc, z_plan)\n",
        "        loss = act_plan_loss + self.beta * reg_loss\n",
        "\n",
        "        # Update metrics \n",
        "        self.valid_loss.update_state(loss)\n",
        "        self.valid_act_with_enc_loss.update_state(act_enc_loss)\n",
        "        self.valid_act_with_plan_loss.update_state(act_plan_loss)\n",
        "        self.valid_reg_loss.update_state(reg_loss)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.train_loss,\n",
        "            self.valid_loss,\n",
        "            self.train_act_with_enc_loss,\n",
        "            self.train_act_with_plan_loss,\n",
        "            self.valid_act_with_enc_loss,\n",
        "            self.valid_act_with_plan_loss,\n",
        "            self.train_reg_loss,\n",
        "            self.valid_reg_loss,\n",
        "            self.valid_position_loss,\n",
        "            self.valid_max_position_loss,\n",
        "            self.valid_rotation_loss,\n",
        "            self.valid_max_rotation_loss,\n",
        "            self.valid_gripper_loss,\n",
        "        ]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWFLZsyahm1X",
        "outputId": "ffd5fc1a-4f68-48b8-af14-6504d3390fec"
      },
      "source": [
        "with strategy.scope():\n",
        "    actor = create_actor(obs_dim=dl.obs_dim, act_dim=dl.act_dim, goal_dim=dl.goal_dim, layer_size=args.actor_layer_size, latent_dim=args.latent_dim)\n",
        "    encoder = create_encoder(obs_dim=dl.obs_dim, act_dim=dl.act_dim, layer_size=args.encoder_layer_size, latent_dim=args.latent_dim)\n",
        "    planner = create_planner(obs_dim=dl.obs_dim, goal_dim=dl.goal_dim, layer_size=args.encoder_layer_size, latent_dim=args.latent_dim)\n",
        "\n",
        "    model = LFPNet(encoder, planner, actor, beta=args.beta)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(args.learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mae', steps_per_execution=1, run_eagerly=False)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not pass `graph_parents`.  They will  no longer be used.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/linalg/linear_operator_lower_triangular.py:167: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Do not pass `graph_parents`.  They will  no longer be used.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2utKU_E_ws2T"
      },
      "source": [
        "class BetaSchedulerCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\" For some reason this doesn't seem to work :/ \"\"\"\n",
        "    def __init__(self, schedule):\n",
        "        super(BetaSchedulerCallback, self).__init__()\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def on_train_batch_begin(self, step, logs=None):\n",
        "        if not hasattr(self.model, \"beta\"):\n",
        "            raise ValueError('Optimizer must have a \"beta\" attribute.')\n",
        "        scheduled_beta = float(self.schedule(step))\n",
        "        # Set the value back to the optimizer before this epoch starts\n",
        "        # tf.keras.backend.set_value(self.model.beta, scheduled_beta)\n",
        "        self.model.beta = scheduled_beta\n",
        "        print(f\"\\nStep {step:05}: Beta is {self.model.beta:.5e}.\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "mRiAk4pzxKSF",
        "outputId": "3ecf5032-16c6-459d-c64b-fc58f53d97df"
      },
      "source": [
        "beta_schedule = lfp.train.BetaScheduler('constant', beta_max=args.beta)\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWFyYFvY7DpW"
      },
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=STORAGE_PATH/'saved_models', monitor='val_total_loss', save_best_only=True, save_weights_only=False, save_freq=1000\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "J6sScbsrgJAv",
        "outputId": "2f1b89dc-f928-4f86-f1e5-088bd0f796d5"
      },
      "source": [
        "wandb.init(project=\"learning-from-play_v2\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.21<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">splendid-universe-205</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sholto/learning-from-play_v2\" target=\"_blank\">https://wandb.ai/sholto/learning-from-play_v2</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/sholto/learning-from-play_v2/runs/2e22apkw\" target=\"_blank\">https://wandb.ai/sholto/learning-from-play_v2/runs/2e22apkw</a><br/>\n",
              "                Run data is saved locally in <code>/content/learning_from_play/wandb/run-20210308_015830-2e22apkw</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f8aa5ed5350>"
            ],
            "text/html": [
              "<h1>Run(2e22apkw)</h1><iframe src=\"https://wandb.ai/sholto/learning-from-play_v2/runs/2e22apkw\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "2fUe22V4lG7T",
        "outputId": "44174068-2099-4e56-e0c9-9610d4197722"
      },
      "source": [
        "from wandb.keras import WandbCallback\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "                    validation_data=valid_dataset,\n",
        "                    epochs=100,\n",
        "                    steps_per_epoch=20,\n",
        "                    validation_steps=1,\n",
        "                    callbacks=[BetaSchedulerCallback(beta_schedule.scheduler), checkpoint_callback, WandbCallback()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "20/20 [==============================] - 159s 3s/step - train_loss: 0.0027 - valid_loss: 0.0000e+00 - train_act_with_enc_loss: 0.0027 - train_act_with_plan_loss: 0.0080 - valid_act_with_enc_loss: 0.0000e+00 - valid_act_with_plan_loss: 0.0000e+00 - reg_loss: 362.3675 - valid_reg_loss: 0.0000e+00 - valid_position_loss: 0.0000e+00 - valid_max_position_loss: 0.0000e+00 - valid_rotation_loss: 0.0000e+00 - valid_max_rotation_loss: 0.0000e+00 - beta: 0.0000e+00 - val_train_loss: 0.0000e+00 - val_valid_loss: 0.0074 - val_train_act_with_enc_loss: 0.0000e+00 - val_train_act_with_plan_loss: 0.0000e+00 - val_valid_act_with_enc_loss: 0.0027 - val_valid_act_with_plan_loss: 0.0074 - val_reg_loss: 0.0000e+00 - val_valid_reg_loss: 358.8278 - val_valid_position_loss: 0.0000e+00 - val_valid_max_position_loss: 0.0000e+00 - val_valid_rotation_loss: 0.0000e+00 - val_valid_max_rotation_loss: 0.0000e+00\n",
            "Epoch 2/100\n",
            "20/20 [==============================] - 56s 3s/step - train_loss: 0.0025 - valid_loss: 0.0000e+00 - train_act_with_enc_loss: 0.0025 - train_act_with_plan_loss: 0.0079 - valid_act_with_enc_loss: 0.0000e+00 - valid_act_with_plan_loss: 0.0000e+00 - reg_loss: 362.8650 - valid_reg_loss: 0.0000e+00 - valid_position_loss: 0.0000e+00 - valid_max_position_loss: 0.0000e+00 - valid_rotation_loss: 0.0000e+00 - valid_max_rotation_loss: 0.0000e+00 - beta: 0.0000e+00 - val_train_loss: 0.0000e+00 - val_valid_loss: 0.0073 - val_train_act_with_enc_loss: 0.0000e+00 - val_train_act_with_plan_loss: 0.0000e+00 - val_valid_act_with_enc_loss: 0.0025 - val_valid_act_with_plan_loss: 0.0073 - val_reg_loss: 0.0000e+00 - val_valid_reg_loss: 355.9492 - val_valid_position_loss: 0.0000e+00 - val_valid_max_position_loss: 0.0000e+00 - val_valid_rotation_loss: 0.0000e+00 - val_valid_max_rotation_loss: 0.0000e+00\n",
            "Epoch 3/100\n",
            "20/20 [==============================] - 52s 3s/step - train_loss: 0.0024 - valid_loss: 0.0000e+00 - train_act_with_enc_loss: 0.0024 - train_act_with_plan_loss: 0.0078 - valid_act_with_enc_loss: 0.0000e+00 - valid_act_with_plan_loss: 0.0000e+00 - reg_loss: 369.5439 - valid_reg_loss: 0.0000e+00 - valid_position_loss: 0.0000e+00 - valid_max_position_loss: 0.0000e+00 - valid_rotation_loss: 0.0000e+00 - valid_max_rotation_loss: 0.0000e+00 - beta: 0.0000e+00 - val_train_loss: 0.0000e+00 - val_valid_loss: 0.0072 - val_train_act_with_enc_loss: 0.0000e+00 - val_train_act_with_plan_loss: 0.0000e+00 - val_valid_act_with_enc_loss: 0.0024 - val_valid_act_with_plan_loss: 0.0072 - val_reg_loss: 0.0000e+00 - val_valid_reg_loss: 370.8773 - val_valid_position_loss: 0.0000e+00 - val_valid_max_position_loss: 0.0000e+00 - val_valid_rotation_loss: 0.0000e+00 - val_valid_max_rotation_loss: 0.0000e+00\n",
            "Epoch 4/100\n",
            "20/20 [==============================] - ETA: 0s - train_loss: 0.0024 - valid_loss: 0.0000e+00 - train_act_with_enc_loss: 0.0024 - train_act_with_plan_loss: 0.0078 - valid_act_with_enc_loss: 0.0000e+00 - valid_act_with_plan_loss: 0.0000e+00 - reg_loss: 383.8192 - valid_reg_loss: 0.0000e+00 - valid_position_loss: 0.0000e+00 - valid_max_position_loss: 0.0000e+00 - valid_rotation_loss: 0.0000e+00 - valid_max_rotation_loss: 0.0000e+00 - beta: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3c4429a7f2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     callbacks=[])#BetaSchedulerCallback(beta_schedule.scheduler), checkpoint_callback, WandbCallback()])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                     )\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1392\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     \"\"\"\n\u001b[1;32m    475\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_test_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw21h14BnX3Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d32c8b-8287-4263-d7ef-b6c32d059e71"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'action_loss': [0.12956753373146057],\n",
              " 'beta': [0.0],\n",
              " 'reg_loss': [1926.2640380859375],\n",
              " 'total_loss': [0.12956753373146057],\n",
              " 'val_action_loss': [0.17088711261749268],\n",
              " 'val_reg_loss': [26089.38671875],\n",
              " 'val_total_loss': [0.17109845578670502]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNeXYIzTUwjg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}