{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import roboticsPlayroomPybullet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/sholto/Desktop/AI/RoboticsPlayroomPybullet/roboticsPlayroomPybullet/envs\n",
      "current_dir=/home/sholto/Desktop/AI/RoboticsPlayroomPybullet/roboticsPlayroomPybullet/envs\n",
      "current_dir=/home/sholto/Desktop/AI/RoboticsPlayroomPybullet/roboticsPlayroomPybullet/envs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sholto/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sholto/Desktop/AI/RoboticsPlayroomPybullet/roboticsPlayroomPybullet/envs\n"
     ]
    }
   ],
   "source": [
    "# Create an \n",
    "env = gym.make('UR5PlayAbsRPY1Obj-v0')\n",
    "# Launch a GUI - don't call this if you want it headless\n",
    "env.render('human')\n",
    "# Reset the env - must be called before using the env as this initialises everything\n",
    "_ = env.reset()\n",
    "# Activate img observations as part of the 'o' return dict\n",
    "env.render('playback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the path of the data dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local setup\n",
      "Working path: /home/sholto/Desktop/AI/learning_from_play\n",
      "Storage path: /home/sholto/Desktop/AI/learning_from_play\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TEST_DATASET = \"UR5_slow_gripper_test\" \n",
    "print('Using local setup')\n",
    "WORKING_PATH = Path().absolute().parent\n",
    "print(f'Working path: {WORKING_PATH}')\n",
    "os.chdir(WORKING_PATH)\n",
    "STORAGE_PATH = WORKING_PATH\n",
    "print(f'Storage path: {STORAGE_PATH}')\n",
    "TRAIN_DATA_PATHS = [STORAGE_PATH/'data'/x for x in [\"UR5\", \"UR5_high_transition\", \"UR5_slow_gripper\"]]\n",
    "TEST_DATA_PATH = STORAGE_PATH/'data'/TEST_DATASET\n",
    "\n",
    "# TRAIN_DATA_PATHS are the big folders which represent a few hours of teleop on different days\n",
    "# Within each of these folders are individual trajectories, which are occasionally reset for breaks \n",
    "# or the block goes out of reach.\n",
    "# Structure\n",
    "# DIR\n",
    " # DIR/obs_act_etc/  - this contains the npz files with obs/acts/ags at 25Hz, all info necessary to train models\n",
    "                # /1\n",
    "                # /2 etc\n",
    " # DIR/states_and_ims/ - this contains the states we can reset to (which contain info like precise contact points\\\n",
    "                        # which is better for deterministic reset and rollout, but large so we only saved these at > 1Hz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the teleop data, showcasing RL support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sholto/Desktop/AI/learning_from_play/data/UR5/states_and_ims/0/ims\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-699e38b7ba1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m# Play out the teleoperated actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# r is sparse -1, 0 and defined in 'playRewardFunc.py' in the env repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AI/RoboticsPlayroomPybullet/roboticsPlayroomPybullet/envs/environments.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mtargetPoses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'achieved_goal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'desired_goal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/AI/RoboticsPlayroomPybullet/roboticsPlayroomPybullet/envs/environments.py\u001b[0m in \u001b[0;36mrunSimulation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateToggles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# so its got both in VR and replay out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 25Hz control at 300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0;31m# Resets goal positions, if a goal is passed in - it will reset the goal to that position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_goal_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy.misc\n",
    "from IPython.display import display, clear_output\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "\n",
    "# The relevant keys from the teleop data  - other keys include ['obs', 'acts', 'achieved_goals', 'joint_poses', 'target_poses', 'acts_quat', 'acts_rpy_rel',  'velocities', 'obs_quat']\n",
    "# Which are useful if you want to act with joint poses, in quaternion space for obs, with velocity etc\n",
    "\n",
    "keys = ['obs', 'acts', 'achieved_goals']\n",
    "full_state_save_interval = 30 # How often we saved the determinstically resettable state (full info e.g. contact points )\n",
    "\n",
    "for DIR in TRAIN_DATA_PATHS:\n",
    "    DIR = str(DIR) # glob/natsorted uses strings\n",
    "    \n",
    "    obs_act_path = DIR+'/obs_act_etc/'\n",
    "\n",
    "    # Each demo is one continuous play trajectory\n",
    "    for demo in natsorted(os.listdir(obs_act_path)):\n",
    "        # Gets the saved states (which we can reset to) \n",
    "        start_points = natsorted(glob.glob(DIR+'/states_and_ims/'+str(demo)+'/env_states/*.bullet'))\n",
    "        # The actual data\n",
    "        traj = np.load(obs_act_path+demo+'/data.npz')\n",
    "        # The actions taken and the states of the non-arm elements of the environment\n",
    "        acts,achieved_goals = traj['acts'], traj['achieved_goals']\n",
    "        # The total length of this recorded trajectory\n",
    "        set_len = len(acts)\n",
    "    \n",
    "        start = 0 \n",
    "        # End is the steps till the next reset state, or the end of the demo\n",
    "        end= min(start+full_state_save_interval, set_len)\n",
    "        print(DIR+'/states_and_ims/'+str(demo)+'/ims')\n",
    "\n",
    "        # Between each state we reset to - roll out the actions. \n",
    "        # A tiny amount of divergence is to be expected, which is why we reset whenever we have the information to do so\n",
    "        for start_point in start_points:\n",
    "\n",
    "            # As the states were saved without subgoals - delete these or we will not be able to reset state \n",
    "            env.delete_sub_goal()\n",
    "            env.p.restoreState(fileName=start_point)\n",
    "            env.instance.updateToggles() # need to do it when when you restore from a state as colors are not saved\n",
    "            \n",
    "            goal = achieved_goals[end] # gets the goal 30 timesteps away\n",
    "            env.reset_goal_pos(goal) # sets the env's goal to this\n",
    "            env.instance.visualise_sub_goal(goal) # visualises it using transparent objects\n",
    "            for i in range(start, end):\n",
    "                # Play out the teleoperated actions\n",
    "                o,r,_,_ = env.step(acts[i])\n",
    "                # r is sparse -1, 0 and defined in 'playRewardFunc.py' in the env repo\n",
    "\n",
    "            start += full_state_save_interval\n",
    "            end = min(start+full_state_save_interval, set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs_quat': array([-1.05122246e-01, -1.58003315e-01, -3.55988974e-03, -9.47304070e-03,\n",
       "        -9.51900929e-02,  1.15734883e-01,  9.88663018e-01,  5.44099584e-02,\n",
       "        -9.76497903e-02,  1.10748991e-01, -9.99999975e-06, -2.50560520e-15,\n",
       "         8.16640219e-15,  9.60237384e-01, -2.79184759e-01, -4.55279015e-02,\n",
       "         4.82954718e-02,  2.99999993e-02,  0.00000000e+00], dtype=float32),\n",
       " 'achieved_goal': array([-9.7649790e-02,  1.1074899e-01, -9.9999997e-06, -2.5056052e-15,\n",
       "         8.1664022e-15,  9.6023738e-01, -2.7918476e-01, -4.5527902e-02,\n",
       "         4.8295472e-02,  2.9999999e-02,  0.0000000e+00], dtype=float32),\n",
       " 'desired_goal': array([-0.12080648,  0.24572381,  0.09474243], dtype=float32),\n",
       " 'controllable_achieved_goal': array([-0.10512225, -0.15800332, -0.00355989,  0.05440996], dtype=float32),\n",
       " 'full_positional_state': array([-1.05122246e-01, -1.58003315e-01, -3.55988974e-03, -9.47304070e-03,\n",
       "        -9.51900929e-02,  1.15734883e-01,  9.88663018e-01,  5.44099584e-02,\n",
       "        -9.76497903e-02,  1.10748991e-01, -9.99999975e-06, -2.50560520e-15,\n",
       "         8.16640219e-15,  9.60237384e-01, -2.79184759e-01, -4.55279015e-02,\n",
       "         4.82954718e-02,  2.99999993e-02,  0.00000000e+00], dtype=float32),\n",
       " 'joints': [-1.305407923559522,\n",
       "  -2.0782402326217735,\n",
       "  -1.5820561001406908,\n",
       "  -1.2378815789558382,\n",
       "  1.6175551368084176,\n",
       "  1.6002646764005428,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'velocity': array([ 0.04474132, -0.1204387 , -0.06955998, -0.14583088, -0.02473012,\n",
       "        -0.48651519]),\n",
       " 'img': array([[[217, 217, 217],\n",
       "         [204, 204, 204],\n",
       "         [221, 221, 221],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [221, 221, 221],\n",
       "         [208, 208, 208]],\n",
       " \n",
       "        [[224, 224, 224],\n",
       "         [211, 211, 211],\n",
       "         [227, 227, 227],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [221, 221, 221],\n",
       "         [208, 208, 208]],\n",
       " \n",
       "        [[227, 227, 227],\n",
       "         [219, 219, 219],\n",
       "         [206, 206, 206],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [222, 222, 222],\n",
       "         [209, 209, 209]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[160, 160, 160],\n",
       "         [162, 162, 162],\n",
       "         [163, 163, 163],\n",
       "         ...,\n",
       "         [207, 181, 156],\n",
       "         [209, 184, 159],\n",
       "         [205, 181, 154]],\n",
       " \n",
       "        [[161, 161, 161],\n",
       "         [162, 162, 162],\n",
       "         [163, 163, 163],\n",
       "         ...,\n",
       "         [207, 183, 157],\n",
       "         [211, 186, 160],\n",
       "         [210, 186, 160]],\n",
       " \n",
       "        [[161, 161, 161],\n",
       "         [162, 162, 162],\n",
       "         [163, 163, 163],\n",
       "         ...,\n",
       "         [212, 188, 161],\n",
       "         [209, 187, 157],\n",
       "         [209, 184, 158]]], dtype=uint8),\n",
       " 'observation': array([-1.05122246e-01, -1.58003314e-01, -3.55988981e-03, -4.15010566e-02,\n",
       "        -1.87119174e-01,  2.36958038e-01,  5.44099570e-02, -9.76497912e-02,\n",
       "         1.10748995e-01, -1.00000000e-05, -2.50560519e-15,  8.16640196e-15,\n",
       "         9.60237406e-01, -2.79184751e-01, -4.55279005e-02,  4.82954735e-02,\n",
       "         3.00000000e-02,  0.00000000e+00]),\n",
       " 'gripper_proprioception': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
